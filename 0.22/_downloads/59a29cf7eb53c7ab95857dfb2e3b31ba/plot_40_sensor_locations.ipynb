{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Working with sensor locations\n\nThis tutorial describes how to read and plot sensor locations, and how\nthe physical location of sensors is handled in MNE-Python.\n   :depth: 2\n\nAs usual we'll start by importing the modules we need and loading some\n`example data <sample-dataset>`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # noqa\nimport mne\n\nsample_data_folder = mne.datasets.sample.data_path()\nsample_data_raw_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n                                    'sample_audvis_raw.fif')\nraw = mne.io.read_raw_fif(sample_data_raw_file, preload=True, verbose=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## About montages and layouts\n\n:class:`Montages <mne.channels.DigMontage>` contain sensor\npositions in 3D (``x``, ``y``, ``z``, in meters), and can be used to set\nthe physical positions of sensors. By specifying the location of sensors\nrelative to the brain, :class:`Montages <mne.channels.DigMontage>` play an\nimportant role in computing the forward solution and computing inverse\nestimates.\n\nIn contrast, :class:`Layouts <mne.channels.Layout>` are *idealized* 2-D\nrepresentations of sensor positions, and are primarily used for arranging\nindividual sensor subplots in a topoplot, or for showing the *approximate*\nrelative arrangement of sensors as seen from above.\n\n## Working with built-in montages\n\nThe 3D coordinates of MEG sensors are included in the raw recordings from MEG\nsystems, and are automatically stored in the ``info`` attribute of the\n:class:`~mne.io.Raw` file upon loading. EEG electrode locations are much more\nvariable because of differences in head shape. Idealized montages for many\nEEG systems are included during MNE-Python installation; these files are\nstored in your ``mne-python`` directory, in the\n:file:`mne/channels/data/montages` folder:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "montage_dir = os.path.join(os.path.dirname(mne.__file__),\n                           'channels', 'data', 'montages')\nprint('\\nBUILT-IN MONTAGE FILES')\nprint('======================')\nprint(sorted(os.listdir(montage_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. sidebar:: Computing sensor locations\n\n    If you are interested in how standard (\"idealized\") EEG sensor positions\n    are computed on a spherical head model, the `eeg_positions`_ repository\n    provides code and documentation to this end.\n\nThese built-in EEG montages can be loaded via\n:func:`mne.channels.make_standard_montage`. Note that when loading via\n:func:`~mne.channels.make_standard_montage`, provide the filename *without*\nits file extension:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ten_twenty_montage = mne.channels.make_standard_montage('standard_1020')\nprint(ten_twenty_montage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once loaded, a montage can be applied to data via one of the instance methods\nsuch as :meth:`raw.set_montage <mne.io.Raw.set_montage>`. It is also possible\nto skip the loading step by passing the filename string directly to the\n:meth:`~mne.io.Raw.set_montage` method. This won't work with our sample\ndata, because it's channel names don't match the channel names in the\nstandard 10-20 montage, so these commands are not run here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# these will be equivalent:\n# raw_1020 = raw.copy().set_montage(ten_twenty_montage)\n# raw_1020 = raw.copy().set_montage('standard_1020')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`Montage <mne.channels.DigMontage>` objects have a\n:meth:`~mne.channels.DigMontage.plot` method for visualization of the sensor\nlocations in 3D; 2D projections are also possible by passing\n``kind='topomap'``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = ten_twenty_montage.plot(kind='3d')\nfig.gca().view_init(azim=70, elev=15)\nten_twenty_montage.plot(kind='topomap', show_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Controlling channel projection (MNE vs EEGLAB)\n\nChannel positions in 2d space are obtained by projecting their actual 3d\npositions using a sphere as a reference. Because ``'standard_1020'`` montage\ncontains realistic, not spherical, channel positions, we will use a different\nmontage to demonstrate controlling how channels are projected to 2d space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\nbiosemi_montage.plot(show_names=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default a sphere  with an origin in ``(0, 0, 0)`` x, y, z coordinates and\nradius of ``0.095`` meters (9.5 cm) is used. You can use a different sphere\nradius by passing a single value to ``sphere`` argument in any function that\nplots channels in 2d (like :meth:`~mne.channels.DigMontage.plot` that we use\nhere, but also for example :func:`mne.viz.plot_topomap`):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "biosemi_montage.plot(show_names=False, sphere=0.07)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To control not only radius, but also the sphere origin, pass a\n``(x, y, z, radius)`` tuple to ``sphere`` argument:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "biosemi_montage.plot(show_names=False, sphere=(0.03, 0.02, 0.01, 0.075))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In mne-python the head center and therefore the sphere center are calculated\nusing fiducial points. Because of this the head circle represents head\ncircumference at the nasion and ear level, and not where it is commonly\nmeasured in 10-20 EEG system: above nasion at T4/T8, T3/T7, Oz, Fz level.\nNotice below that by default T7 and Oz channels are placed within the head\ncircle, not on the head outline:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "biosemi_montage.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you have previous EEGLAB experience you may prefer its convention to\nrepresent 10-20 head circumference with the head circle. To get EEGLAB-like\nchannel layout you would have to move the sphere origin a few centimeters\nup on the z dimension:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "biosemi_montage.plot(sphere=(0, 0, 0.035, 0.094))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of approximating the EEGLAB-esque sphere location as above, you can\ncalculate the sphere origin from position of Oz, Fpz, T3/T7 or T4/T8\nchannels. This is easier once the montage has been applied to the data and\nchannel positions are in the head space - see\n`this example <ex-topomap-eeglab-style>`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Reading sensor digitization files\n\nIn the sample data, setting the digitized EEG montage was done prior to\nsaving the :class:`~mne.io.Raw` object to disk, so the sensor positions are\nalready incorporated into the ``info`` attribute of the :class:`~mne.io.Raw`\nobject (see the documentation of the reading functions and\n:meth:`~mne.io.Raw.set_montage` for details on how that works). Because of\nthat, we can plot sensor locations directly from the :class:`~mne.io.Raw`\nobject using the :meth:`~mne.io.Raw.plot_sensors` method, which provides\nsimilar functionality to\n:meth:`montage.plot() <mne.channels.DigMontage.plot>`.\n:meth:`~mne.io.Raw.plot_sensors` also allows channel selection by type, can\ncolor-code channels in various ways (by default, channels listed in\n``raw.info['bads']`` will be plotted in red), and allows drawing into an\nexisting matplotlib ``axes`` object (so the channel positions can easily be\nmade as a subplot in a multi-panel figure):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\nax2d = fig.add_subplot(121)\nax3d = fig.add_subplot(122, projection='3d')\nraw.plot_sensors(ch_type='eeg', axes=ax2d)\nraw.plot_sensors(ch_type='eeg', axes=ax3d, kind='3d')\nax3d.view_init(azim=70, elev=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's probably evident from the 2D topomap above that there is some\nirregularity in the EEG sensor positions in the `sample dataset\n<sample-dataset>` \u2014 this is because the sensor positions in that dataset are\ndigitizations of the sensor positions on an actual subject's head, rather\nthan idealized sensor positions based on a spherical head model. Depending on\nwhat system was used to digitize the electrode positions (e.g., a Polhemus\nFastrak digitizer), you must use different montage reading functions (see\n`dig-formats`). The resulting :class:`montage <mne.channels.DigMontage>`\ncan then be added to :class:`~mne.io.Raw` objects by passing it to the\n:meth:`~mne.io.Raw.set_montage` method (just as we did above with the name of\nthe idealized montage ``'standard_1020'``). Once loaded, locations can be\nplotted with :meth:`~mne.channels.DigMontage.plot` and saved with\n:meth:`~mne.channels.DigMontage.save`, like when working with a standard\nmontage.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>When setting a montage with :meth:`~mne.io.Raw.set_montage`\n    the measurement info is updated in two places (the ``chs``\n    and ``dig`` entries are updated). See `tut-info-class`.\n    ``dig`` may contain HPI, fiducial, or head shape points in\n    addition to electrode locations.</p></div>\n\n\n## Rendering sensor position with mayavi\n\nIt is also possible to render an image of a MEG sensor helmet in 3D, using\nmayavi instead of matplotlib, by calling :func:`mne.viz.plot_alignment`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = mne.viz.plot_alignment(raw.info, trans=None, dig=False, eeg=False,\n                             surfaces=[], meg=['helmet', 'sensors'],\n                             coord_frame='meg')\nmne.viz.set_3d_view(fig, azimuth=50, elevation=90, distance=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":func:`~mne.viz.plot_alignment` requires an :class:`~mne.Info` object, and\ncan also render MRI surfaces of the scalp, skull, and brain (by passing\nkeywords like ``'head'``, ``'outer_skull'``, or ``'brain'`` to the\n``surfaces`` parameter) making it useful for `assessing coordinate frame\ntransformations <plot_source_alignment>`. For examples of various uses of\n:func:`~mne.viz.plot_alignment`, see `plot_montage`,\n:doc:`../../auto_examples/visualization/plot_eeg_on_scalp`, and\n:doc:`../../auto_examples/visualization/plot_meg_sensors`.\n\n\n## Working with layout files\n\nAs with montages, many layout files are included during MNE-Python\ninstallation, and are stored in the :file:`mne/channels/data/layouts` folder:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layout_dir = os.path.join(os.path.dirname(mne.__file__),\n                          'channels', 'data', 'layouts')\nprint('\\nBUILT-IN LAYOUT FILES')\nprint('=====================')\nprint(sorted(os.listdir(layout_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may have noticed that the file formats and filename extensions of the\nbuilt-in layout and montage files vary considerably. This reflects different\nmanufacturers' conventions; to make loading easier the montage and layout\nloading functions in MNE-Python take the filename *without its extension* so\nyou don't have to keep track of which file format is used by which\nmanufacturer.\n\nTo load a layout file, use the :func:`mne.channels.read_layout` function, and\nprovide the filename *without* its file extension. You can then visualize the\nlayout using its :meth:`~mne.channels.Layout.plot` method, or (equivalently)\nby passing it to :func:`mne.viz.plot_layout`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "biosemi_layout = mne.channels.read_layout('biosemi')\nbiosemi_layout.plot()  # same result as: mne.viz.plot_layout(biosemi_layout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to the ``picks`` argument for selecting channels from\n:class:`~mne.io.Raw` objects, the :meth:`~mne.channels.Layout.plot` method of\n:class:`~mne.channels.Layout` objects also has a ``picks`` argument. However,\nbecause layouts only contain information about sensor name and location (not\nsensor type), the :meth:`~mne.channels.Layout.plot` method only allows\npicking channels by index (not by name or by type). Here we find the indices\nwe want using :func:`numpy.where`; selection by name or type is possible via\n:func:`mne.pick_channels` or :func:`mne.pick_types`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "midline = np.where([name.endswith('z') for name in biosemi_layout.names])[0]\nbiosemi_layout.plot(picks=midline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're working with a :class:`~mne.io.Raw` object that already has sensor\npositions incorporated, you can create a :class:`~mne.channels.Layout` object\nwith either the :func:`mne.channels.make_eeg_layout` function or\n(equivalently) the :func:`mne.channels.find_layout` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "layout_from_raw = mne.channels.make_eeg_layout(raw.info)\n# same result as: mne.channels.find_layout(raw.info, ch_type='eeg')\nlayout_from_raw.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There is no corresponding ``make_meg_layout`` function because sensor\n    locations are fixed in a MEG system (unlike in EEG, where the sensor caps\n    deform to fit each subject's head). Thus MEG layouts are consistent for a\n    given system and you can simply load them with\n    :func:`mne.channels.read_layout`, or use :func:`mne.channels.find_layout`\n    with the ``ch_type`` parameter, as shown above for EEG.</p></div>\n\nAll :class:`~mne.channels.Layout` objects have a\n:meth:`~mne.channels.Layout.save` method that allows writing layouts to disk,\nin either :file:`.lout` or :file:`.lay` format (which format gets written is\ninferred from the file extension you pass to the method's ``fname``\nparameter). The choice between :file:`.lout` and :file:`.lay` format only\nmatters if you need to load the layout file in some other software\n(MNE-Python can read either format equally well).\n\n\n.. LINKS\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}