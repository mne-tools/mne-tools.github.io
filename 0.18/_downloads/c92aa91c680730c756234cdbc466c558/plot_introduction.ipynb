{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nOverview of MEG/EEG analysis with MNE-Python\n============================================\n\nThis tutorial covers the basic EEG/MEG pipeline for event-related analysis:\nloading data, epoching, averaging, plotting, and estimating cortical activity\nfrom sensor data. It introduces the core MNE-Python data structures\n:class:`~mne.io.Raw`, :class:`~mne.Epochs`, :class:`~mne.Evoked`, and\n:class:`~mne.SourceEstimate`, and covers a lot of ground fairly quickly (at the\nexpense of depth). Subsequent tutorials address each of these topics in greater\ndetail.\n   :depth: 1\n\nWe begin by importing the necessary Python modules:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport mne"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading data\n^^^^^^^^^^^^\n\nMNE-Python data structures are based around the FIF file format from\nNeuromag, but there are reader functions for `a wide variety of other\ndata formats <data-formats>`. MNE-Python also has interfaces to a\nvariety of :doc:`publicly available datasets <../../manual/datasets_index>`,\nwhich MNE-Python can download and manage for you.\n\nWe'll start this tutorial by loading one of the example datasets (called\n\"`sample-dataset`\"), which contains EEG and MEG data from one subject\nperforming an audiovisual experiment, along with structural MRI scans for\nthat subject. The :func:`mne.datasets.sample.data_path` function will\nautomatically download the dataset if it isn't found in one of the expected\nlocations, then return the directory path to the dataset (see the\ndocumentation of :func:`~mne.datasets.sample.data_path` for a list of places\nit checks before downloading). Note also that for this tutorial to run\nsmoothly on our servers, we're using a filtered and downsampled version of\nthe data (:file:`sample_audvis_filt-0-40_raw.fif`), but an unfiltered version\n(:file:`sample_audvis_raw.fif`) is also included in the sample dataset and\ncould be substituted here when running the tutorial locally.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample_data_folder = mne.datasets.sample.data_path()\nsample_data_raw_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n                                    'sample_audvis_filt-0-40_raw.fif')\nraw = mne.io.read_raw_fif(sample_data_raw_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, :func:`~mne.io.read_raw_fif` displays some information about the\nfile it's loading; for example, here it tells us that there are four\n\"projection items\" in the file along with the recorded data; those are\n:term:`SSP projectors <projector>` calculated to remove environmental noise\nfrom the MEG signals, plus a projector to mean-reference the EEG channels;\nthese are discussed\nin a later tutorial. In addition to the information displayed during loading,\nyou can get a glimpse of the basic details of a :class:`~mne.io.Raw` object\nby printing it; even more is available by printing its ``info`` attribute\n(a :class:`dictionary-like object <mne.Info>` that is preserved across\n:class:`~mne.io.Raw`, :class:`~mne.Epochs`, and :class:`~mne.Evoked`\nobjects). The ``info`` data structure keeps track of channel locations,\napplied filters, projectors, etc. Notice especially the ``chs`` entry,\nshowing that MNE-Python detects different sensor types and handles each\nappropriately.\n\n.. TODO edit prev. paragraph when projectors tutorial is added: ...those are\n    discussed in the tutorial `projectors-tutorial`. (or whatever link)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(raw)\nprint(raw.info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":class:`~mne.io.Raw` objects also have several built-in plotting methods;\nhere we show the power spectral density (PSD) for each sensor type with\n:meth:`~mne.io.Raw.plot_psd`, as well as a plot of the raw sensor traces with\n:meth:`~mne.io.Raw.plot`. In the PSD plot, we'll only plot frequencies below\n50 Hz (since our data are low-pass filtered at 40 Hz). In interactive Python\nsessions, :meth:`~mne.io.Raw.plot` is interactive and allows scrolling,\nscaling, bad channel marking, annotation, projector toggling, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.plot_psd(fmax=50)\nraw.plot(duration=5, n_channels=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing\n^^^^^^^^^^^^^\n\nMNE-Python supports a variety of preprocessing approaches and techniques\n(maxwell filtering, signal-space projection, independent components analysis,\nfiltering, downsampling, etc); see the full list of capabilities in the\n:mod:`mne.preprocessing` and :mod:`mne.filter` submodules. Here we'll clean\nup our data by performing independent components analysis\n(:class:`~mne.preprocessing.ICA`); for brevity we'll skip the steps that\nhelped us determined which components best capture the artifacts (see\n:doc:`../preprocessing/plot_artifacts_correction_ica` for a detailed\nwalk-through of that process).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# set up and fit the ICA\nica = mne.preprocessing.ICA(n_components=20, random_state=97, max_iter=800)\nica.fit(raw)\nica.exclude = [1, 2]  # details on how we picked these are omitted here\nica.plot_properties(raw, picks=ica.exclude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we're confident about which component(s) we want to remove, we pass them\nas the ``exclude`` parameter and then apply the ICA to the raw signal. The\n:meth:`~mne.preprocessing.ICA.apply` method requires the raw data to be\nloaded into memory (by default it's only read from disk as-needed), so we'll\nuse :meth:`~mne.io.Raw.load_data` first. We'll also make a copy of the\n:class:`~mne.io.Raw` object so we can compare the signal before and after\nartifact removal side-by-side:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "orig_raw = raw.copy()\nraw.load_data()\nica.apply(raw)\n\n# show some frontal channels to clearly illustrate the artifact removal\nchs = ['MEG 0111', 'MEG 0121', 'MEG 0131', 'MEG 0211', 'MEG 0221', 'MEG 0231',\n       'MEG 0311', 'MEG 0321', 'MEG 0331', 'MEG 1511', 'MEG 1521', 'MEG 1531',\n       'EEG 001', 'EEG 002', 'EEG 003', 'EEG 004', 'EEG 005', 'EEG 006',\n       'EEG 007', 'EEG 008']\nchan_idxs = [raw.ch_names.index(ch) for ch in chs]\norig_raw.plot(order=chan_idxs, start=12, duration=4)\nraw.plot(order=chan_idxs, start=12, duration=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Detecting experimental events\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe sample dataset includes several :term:`\"STIM\" channels <stim channel>`\nthat recorded electrical\nsignals sent from the stimulus delivery computer (as brief DC shifts /\nsquarewave pulses). These pulses (often called \"triggers\") are used in this\ndataset to mark experimental events: stimulus onset, stimulus type, and\nparticipant response (button press). The individual STIM channels are\ncombined onto a single channel, in such a way that voltage\nlevels on that channel can be unambiguously decoded as a particular event\ntype. On older Neuromag systems (such as that used to record the sample data)\nthis summation channel was called ``STI 014``, so we can pass that channel\nname to the :func:`mne.find_events` function to recover the timing and\nidentity of the stimulus events.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events = mne.find_events(raw, stim_channel='STI 014')\nprint(events[:5])  # show the first 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting events array is an ordinary 3-column :class:`NumPy array\n<numpy.ndarray>`, with sample number in the first column and integer event ID\nin the last column; the middle column is usually ignored. Rather than keeping\ntrack of integer event IDs, we can provide an *event dictionary* that maps\nthe integer IDs to experimental conditions or events. In this dataset, the\nmapping looks like this:\n\n\n+----------+----------------------------------------------------------+\n| Event ID | Condition                                                |\n+==========+==========================================================+\n| 1        | auditory stimulus (tone) to the left ear                 |\n+----------+----------------------------------------------------------+\n| 2        | auditory stimulus (tone) to the right ear                |\n+----------+----------------------------------------------------------+\n| 3        | visual stimulus (checkerboard) to the left visual field  |\n+----------+----------------------------------------------------------+\n| 4        | visual stimulus (checkerboard) to the right visual field |\n+----------+----------------------------------------------------------+\n| 5        | smiley face (catch trial)                                |\n+----------+----------------------------------------------------------+\n| 32       | subject button press                                     |\n+----------+----------------------------------------------------------+\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "event_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,\n              'visual/right': 4, 'smiley': 5, 'buttonpress': 32}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Event dictionaries like this one are used when extracting epochs from\ncontinuous data; the ``/`` character in the dictionary keys allows pooling\nacross conditions by requesting partial condition descriptors (i.e.,\nrequesting ``'auditory'`` will select all epochs with Event IDs 1 and 2;\nrequesting ``'left'`` will select all epochs with Event IDs 1 and 3). An\nexample of this is shown in the next section. There is also a convenient\n:func:`~mne.viz.plot_events` function for visualizing the distribution of\nevents across the duration of the recording (to make sure event detection\nworked as expected). Here we'll also make use of the :class:`~mne.Info`\nattribute to get the sampling frequency of the recording (so our x-axis will\nbe in seconds instead of in samples).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = mne.viz.plot_events(events, event_id=event_dict, sfreq=raw.info['sfreq'])\nfig.subplots_adjust(right=0.7)  # make room for the legend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For paradigms that are not event-related (e.g., analysis of resting-state\ndata), you can extract regularly spaced (possibly overlapping) spans of data\nby creating events using :func:`mne.make_fixed_length_events` and then\nproceeding with epoching as described in the next section.\n\n\nEpoching continuous data\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :class:`~mne.io.Raw` object and the events array are the bare minimum\nneeded to create an :class:`~mne.Epochs` object, which we create with the\n:class:`mne.Epochs` class constructor. Here we'll also specify some data\nquality constraints: we'll reject any epoch where peak-to-peak signal\namplitude is beyond reasonable limits for that channel type. This is done\nwith a *rejection dictionary*; you may include or omit thresholds for any of\nthe channel types present in your data. The values given here are reasonable\nfor this particular dataset, but may need to be adapted for different\nhardware or recording conditions. For a more automated approach, consider\nusing the `autoreject package`_.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reject_criteria = dict(mag=4000e-15,     # 4000 fT\n                       grad=4000e-13,    # 4000 fT/cm\n                       eeg=150e-6,       # 150 \u03bcV\n                       eog=250e-6)       # 250 \u03bcV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also pass the event dictionary as the ``event_id`` parameter (so we can\nwork with easy-to-pool event labels instead of the integer event IDs), and\nspecify ``tmin`` and ``tmax`` (the time relative to each event at which to\nstart and end each epoch). As mentioned above, by default\n:class:`~mne.io.Raw` and :class:`~mne.Epochs` data aren't loaded into memory\n(they're accessed from disk only when needed), but here we'll force loading\ninto memory using the ``preload=True`` parameter so that we can see the\nresults of the rejection criteria being applied:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.2, tmax=0.5,\n                    reject=reject_criteria, preload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we'll pool across left/right stimulus presentations so we can compare\nauditory versus visual responses. To avoid biasing our signals to the\nleft or right, we'll use :meth:`~mne.Epochs.equalize_event_counts` first to\nrandomly sample epochs from each condition to match the number of epochs\npresent in the condition with the fewest good epochs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conds_we_care_about = ['auditory/left', 'auditory/right',\n                       'visual/left', 'visual/right']\nepochs.equalize_event_counts(conds_we_care_about)  # this operates in-place\naud_epochs = epochs['auditory']\nvis_epochs = epochs['visual']\ndel raw, epochs  # free up memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Like :class:`~mne.io.Raw` objects, :class:`~mne.Epochs` objects also have a\nnumber of built-in plotting methods. One is :meth:`~mne.Epochs.plot_image`,\nwhich shows each epoch as one row of an image map, with color representing\nsignal magnitude; the average evoked response and the sensor location are\nshown below the image:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aud_epochs.plot_image(picks=['MEG 1332', 'EEG 021'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Both :class:`~mne.io.Raw` and :class:`~mne.Epochs` objects have\n    :meth:`~mne.Epochs.get_data` methods that return the underlying data\n    as a :class:`NumPy array <numpy.ndarray>`. Both methods have a ``picks``\n    parameter for subselecting which channel(s) to return; ``raw.get_data()``\n    has additional parameters for restricting the time domain. The resulting\n    matrices have dimension ``(n_channels, n_times)`` for\n    :class:`~mne.io.Raw` and ``(n_epochs, n_channels, n_times)`` for\n    :class:`~mne.Epochs`.</p></div>\n\n\nTime-frequency analysis\n^^^^^^^^^^^^^^^^^^^^^^^\n\nThe :mod:`mne.time_frequency` submodule provides implementations of several\nalgorithms to compute time-frequency representations, power spectral density,\nand cross-spectral density. Here, for example, we'll compute for the auditory\nepochs the induced power at different frequencies and times, using Morlet\nwavelets. On this dataset the result is not especially informative (it just\nshows the evoked \"auditory N100\" response); see `here\n<inter-trial-coherence>` for a more extended example on a dataset with richer\nfrequency content.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "frequencies = np.arange(7, 30, 3)\npower = mne.time_frequency.tfr_morlet(aud_epochs, n_cycles=2, return_itc=False,\n                                      freqs=frequencies, decim=3)\npower.plot(['MEG 1332'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimating evoked responses\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nNow that we have our conditions in ``aud_epochs`` and ``vis_epochs``, we can\nget an estimate of evoked responses to auditory versus visual stimuli by\naveraging together the epochs in each condition. This is as simple as calling\nthe :meth:`~mne.Epochs.average` method on the :class:`~mne.Epochs` object,\nand then using a function from the :mod:`mne.viz` module to compare the\nglobal field power for each sensor type of the two :class:`~mne.Evoked`\nobjects:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aud_evoked = aud_epochs.average()\nvis_evoked = vis_epochs.average()\n\nmne.viz.plot_compare_evokeds(dict(auditory=aud_evoked, visual=vis_evoked),\n                             show_legend='upper left',\n                             show_sensors='upper right')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also get a more detailed view of each :class:`~mne.Evoked` object\nusing other plotting methods such as :meth:`~mne.Evoked.plot_joint` or\n:meth:`~mne.Evoked.plot_topomap`. Here we'll examine just the EEG channels,\nand see the classic auditory evoked N100-P200 pattern over dorso-frontal\nelectrodes, then plot scalp topographies at some additional arbitrary times:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 13\naud_evoked.plot_joint(picks='eeg')\naud_evoked.plot_topomap(times=[0., 0.08, 0.1, 0.12, 0.2], ch_type='eeg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evoked objects can also be combined to show contrasts between conditions,\nusing the :func:`mne.combine_evoked` function. A simple difference can be\ngenerated by negating one of the :class:`~mne.Evoked` objects passed into the\nfunction. We'll then plot the difference wave at each sensor using\n:meth:`~mne.Evoked.plot_topo`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evoked_diff = mne.combine_evoked([aud_evoked, -vis_evoked], weights='equal')\nevoked_diff.pick_types('mag').plot_topo(color='r', legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inverse modeling\n^^^^^^^^^^^^^^^^\n\nFinally, we can estimate the origins of the evoked activity by projecting the\nsensor data into this subject's :term:`source space` (a set of points either\non the cortical surface or within the cortical volume of that subject, as\nestimated by structural MRI scans). MNE-Python supports lots of ways of doing\nthis (dynamic statistical parametric mapping, dipole fitting, beamformers,\netc.); here we'll use minimum-norm estimation (MNE) to generate a continuous\nmap of activation constrained to the cortical surface. MNE uses a linear\n:term:`inverse operator` to project EEG+MEG sensor measurements into the\nsource space. The inverse operator is computed from the\n:term:`forward solution` for this subject and an estimate of `the\ncovariance of sensor measurements <tut_compute_covariance>`. For this\ntutorial we'll skip those computational steps and load a pre-computed inverse\noperator from disk (it's included with the `sample data\n<sample-dataset>`). Because this \"inverse problem\" is underdetermined (there\nis no unique solution), here we further constrain the solution by providing a\nregularization parameter specifying the relative smoothness of the current\nestimates in terms of a signal-to-noise ratio (where \"noise\" here is akin to\nbaseline activity level across all of cortex).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load inverse operator\ninverse_operator_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n                                     'sample_audvis-meg-oct-6-meg-inv.fif')\ninv_operator = mne.minimum_norm.read_inverse_operator(inverse_operator_file)\n# set signal-to-noise ratio (SNR) to compute regularization parameter (\u03bb\u00b2)\nsnr = 3.\nlambda2 = 1. / snr ** 2\n# generate the source time course (STC)\nstc = mne.minimum_norm.apply_inverse(vis_evoked, inv_operator,\n                                     lambda2=lambda2,\n                                     method='MNE')  # or dSPM, sLORETA, eLORETA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, in order to plot the source estimate on the subject's cortical\nsurface we'll also need the path to the sample subject's structural MRI files\n(the ``subjects_dir``):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# path to subjects' MRI files\nsubjects_dir = os.path.join(sample_data_folder, 'subjects')\n# plot\nstc.plot(initial_time=0.1, hemi='split', views=['lat', 'med'],\n         subjects_dir=subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The remaining tutorials have *much more detail* on each of these topics (as\nwell as many other capabilities of MNE-Python not mentioned here:\nconnectivity analysis, encoding/decoding models, lots more visualization\noptions, etc). Read on to learn more!\n\n\n.. LINKS\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}