<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Overview of MEG/EEG analysis with MNE-Python &#8212; MNE 0.18.2 documentation</title>
    <link rel="stylesheet" href="../../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/bootstrap_divs.css" />
    <link rel="stylesheet" href="../../_static/reset-syntax.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <script type="text/javascript" src="../../_static/bootstrap_divs.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />

    <script type="text/javascript" src="../../_static/copybutton.js"></script>


    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37225609-1']);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>


    <link rel="stylesheet" href="../../_static/style.css " type="text/css" />
    <link rel="stylesheet" href="../../_static/font-awesome.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/flag-icon.css" type="text/css" />



    <script type="text/javascript">
    !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);
    js.id=id;js.src="https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");
    </script>



    <script type="text/javascript">
    (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
    })();
    </script>



<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">
<script type="text/javascript" src="../../_static/js/jquery-1.11.0.min.js "></script>
<script type="text/javascript" src="../../_static/js/jquery-fix.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-3.3.7/js/bootstrap.min.js "></script>
<script type="text/javascript" src="../../_static/bootstrap-sphinx.js "></script>



  </head><body>





  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../../index.html"><span><img src="../../_static/mne_logo_small.png"></span>
           </a>
        <span class="navbar-text navbar-version pull-left"><b>0.18.2</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../../getting_started.html">Install</a></li>
                <li><a href="../../documentation.html">Documentation</a></li>
                <li><a href="../../python_reference.html">API</a></li>
                <li><a href="../../glossary.html">Glossary</a></li>
                <li><a href="../../auto_examples/index.html">Examples</a></li>
                <li><a href="../../contributing.html">Contribute</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
<div class="navbar-form navbar-right navbar-btn dropdown btn-group-sm" style="margin-left: 20px; margin-top: 5px; margin-bottom: 5px">
  <button type="button" class="btn btn-primary navbar-btn dropdown-toggle" id="dropdownMenu1" data-toggle="dropdown">
    v0.18.2
    <span class="caret"></span>
  </button>
  <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
    <li><a href="https://mne-tools.github.io/dev/index.html">Development</a></li>
    <li><a href="https://mne-tools.github.io/stable/index.html">v0.18 (stable)</a></li>
    <li><a href="https://mne-tools.github.io/0.17/index.html">v0.17</a></li>
    <li><a href="https://mne-tools.github.io/0.16/index.html">v0.16</a></li>
    <li><a href="https://mne-tools.github.io/0.15/index.html">v0.15</a></li>
    <li><a href="https://mne-tools.github.io/0.14/index.html">v0.14</a></li>
    <li><a href="https://mne-tools.github.io/0.13/index.html">v0.13</a></li>
    <li><a href="https://mne-tools.github.io/0.12/index.html">v0.12</a></li>
    <li><a href="https://mne-tools.github.io/0.11/index.html">v0.11</a></li>
  </ul>
</div>


            
<form class="navbar-form navbar-right" action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          

        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../../index.html">
              <img class="logo" src="../../_static/mne_logo_small.png" alt="Logo"/>
            </a></p><ul>
<li><a class="reference internal" href="#">Overview of MEG/EEG analysis with MNE-Python</a><ul>
<li><a class="reference internal" href="#loading-data">Loading data</a></li>
<li><a class="reference internal" href="#preprocessing">Preprocessing</a></li>
<li><a class="reference internal" href="#detecting-experimental-events">Detecting experimental events</a></li>
<li><a class="reference internal" href="#epoching-continuous-data">Epoching continuous data</a></li>
<li><a class="reference internal" href="#time-frequency-analysis">Time-frequency analysis</a></li>
<li><a class="reference internal" href="#estimating-evoked-responses">Estimating evoked responses</a></li>
<li><a class="reference internal" href="#inverse-modeling">Inverse modeling</a></li>
</ul>
</li>
</ul>

<form action="../../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="body col-md-12 content" role="main">
      
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-auto-tutorials-intro-plot-introduction-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="overview-of-meg-eeg-analysis-with-mne-python">
<span id="sphx-glr-auto-tutorials-intro-plot-introduction-py"></span><h1>Overview of MEG/EEG analysis with MNE-Python<a class="headerlink" href="#overview-of-meg-eeg-analysis-with-mne-python" title="Permalink to this headline">Â¶</a></h1>
<p>This tutorial covers the basic EEG/MEG pipeline for event-related analysis:
loading data, epoching, averaging, plotting, and estimating cortical activity
from sensor data. It introduces the core MNE-Python data structures
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a>, <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a>, <a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked" title="mne.Evoked"><code class="xref py py-class docutils literal notranslate"><span class="pre">Evoked</span></code></a>, and
<a class="reference internal" href="../../generated/mne.SourceEstimate.html#mne.SourceEstimate" title="mne.SourceEstimate"><code class="xref py py-class docutils literal notranslate"><span class="pre">SourceEstimate</span></code></a>, and covers a lot of ground fairly quickly (at the
expense of depth). Subsequent tutorials address each of these topics in greater
detail.</p>
<div class="contents local topic" id="page-contents">
<p class="topic-title first">Page contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#loading-data" id="id1">Loading data</a></p></li>
<li><p><a class="reference internal" href="#preprocessing" id="id2">Preprocessing</a></p></li>
<li><p><a class="reference internal" href="#detecting-experimental-events" id="id3">Detecting experimental events</a></p></li>
<li><p><a class="reference internal" href="#epoching-continuous-data" id="id4">Epoching continuous data</a></p></li>
<li><p><a class="reference internal" href="#time-frequency-analysis" id="id5">Time-frequency analysis</a></p></li>
<li><p><a class="reference internal" href="#estimating-evoked-responses" id="id6">Estimating evoked responses</a></p></li>
<li><p><a class="reference internal" href="#inverse-modeling" id="id7">Inverse modeling</a></p></li>
</ul>
</div>
<p>We begin by importing the necessary Python modules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">mne</span>
</pre></div>
</div>
<div class="section" id="loading-data">
<h2><a class="toc-backref" href="#id1">Loading data</a><a class="headerlink" href="#loading-data" title="Permalink to this headline">Â¶</a></h2>
<p>MNE-Python data structures are based around the FIF file format from
Neuromag, but there are reader functions for <a class="reference internal" href="../../manual/io.html#data-formats"><span class="std std-ref">a wide variety of other
data formats</span></a>. MNE-Python also has interfaces to a
variety of <a class="reference internal" href="../../manual/datasets_index.html"><span class="doc">publicly available datasets</span></a>,
which MNE-Python can download and manage for you.</p>
<p>Weâll start this tutorial by loading one of the example datasets (called
â<a class="reference internal" href="../../manual/datasets_index.html#sample-dataset"><span class="std std-ref">Sample</span></a>â), which contains EEG and MEG data from one subject
performing an audiovisual experiment, along with structural MRI scans for
that subject. The <a class="reference internal" href="../../generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path" title="mne.datasets.sample.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.sample.data_path()</span></code></a> function will
automatically download the dataset if it isnât found in one of the expected
locations, then return the directory path to the dataset (see the
documentation of <a class="reference internal" href="../../generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path" title="mne.datasets.sample.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">data_path()</span></code></a> for a list of places
it checks before downloading). Note also that for this tutorial to run
smoothly on our servers, weâre using a filtered and downsampled version of
the data (<code class="file docutils literal notranslate"><span class="pre">sample_audvis_filt-0-40_raw.fif</span></code>), but an unfiltered version
(<code class="file docutils literal notranslate"><span class="pre">sample_audvis_raw.fif</span></code>) is also included in the sample dataset and
could be substituted here when running the tutorial locally.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sample_data_folder</span> <span class="o">=</span> <a href="../../generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path" title="View documentation for mne.datasets.sample.data_path"><span class="n">mne</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">sample</span><span class="o">.</span><span class="n">data_path</span></a><span class="p">()</span>
<span class="n">sample_data_raw_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sample_data_folder</span><span class="p">,</span> <span class="s1">&#39;MEG&#39;</span><span class="p">,</span> <span class="s1">&#39;sample&#39;</span><span class="p">,</span>
                                    <span class="s1">&#39;sample_audvis_filt-0-40_raw.fif&#39;</span><span class="p">)</span>
<span class="n">raw</span> <span class="o">=</span> <a href="../../generated/mne.io.read_raw_fif.html#mne.io.read_raw_fif" title="View documentation for mne.io.read_raw_fif"><span class="n">mne</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">read_raw_fif</span></a><span class="p">(</span><span class="n">sample_data_raw_file</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Opening raw data file /home/circleci/mne_data/MNE-sample-data/MEG/sample/sample_audvis_filt-0-40_raw.fif...
    Read a total of 4 projection items:
        PCA-v1 (1 x 102)  idle
        PCA-v2 (1 x 102)  idle
        PCA-v3 (1 x 102)  idle
        Average EEG reference (1 x 60)  idle
    Range : 6450 ... 48149 =     42.956 ...   320.665 secs
Ready.
Current compensation grade : 0
</pre></div>
</div>
<p>By default, <a class="reference internal" href="../../generated/mne.io.read_raw_fif.html#mne.io.read_raw_fif" title="mne.io.read_raw_fif"><code class="xref py py-func docutils literal notranslate"><span class="pre">read_raw_fif()</span></code></a> displays some information about the
file itâs loading; for example, here it tells us that there are four
âprojection itemsâ in the file along with the recorded data; those are
<a class="reference internal" href="../../glossary.html#term-projector"><span class="xref std std-term">SSP projectors</span></a> calculated to remove environmental noise
from the MEG signals, plus a projector to mean-reference the EEG channels;
these are discussed
in a later tutorial. In addition to the information displayed during loading,
you can get a glimpse of the basic details of a <a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> object
by printing it; even more is available by printing its <code class="docutils literal notranslate"><span class="pre">info</span></code> attribute
(a <a class="reference internal" href="../../generated/mne.Info.html#mne.Info" title="mne.Info"><code class="xref py py-class docutils literal notranslate"><span class="pre">dictionary-like</span> <span class="pre">object</span></code></a> that is preserved across
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a>, <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a>, and <a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked" title="mne.Evoked"><code class="xref py py-class docutils literal notranslate"><span class="pre">Evoked</span></code></a>
objects). The <code class="docutils literal notranslate"><span class="pre">info</span></code> data structure keeps track of channel locations,
applied filters, projectors, etc. Notice especially the <code class="docutils literal notranslate"><span class="pre">chs</span></code> entry,
showing that MNE-Python detects different sensor types and handles each
appropriately.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw</span><span class="o">.</span><span class="n">info</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;Raw  |  sample_audvis_filt-0-40_raw.fif, n_channels x n_times : 376 x 41700 (277.7 sec), ~3.7 MB, data not loaded&gt;
&lt;Info | 19 non-empty fields
    bads : list | MEG 2443, EEG 053
    ch_names : list | MEG 0113, MEG 0112, MEG 0111, MEG 0122, MEG 0123, ...
    chs : list | 376 items (GRAD: 204, MAG: 102, STIM: 9, EEG: 60, EOG: 1)
    comps : list | 0 items
    custom_ref_applied : bool | False
    dev_head_t : Transform | 3 items
    dig : list | 146 items (3 Cardinal, 4 HPI, 61 EEG, 78 Extra)
    events : list | 0 items
    file_id : dict | 4 items
    highpass : float | 0.10000000149011612 Hz
    hpi_meas : list | 1 items
    hpi_results : list | 1 items
    lowpass : float | 40.0 Hz
    meas_date : tuple | 2002-12-03 19:01:10 GMT
    meas_id : dict | 4 items
    nchan : int | 376
    proc_history : list | 0 items
    projs : list | PCA-v1: off, PCA-v2: off, PCA-v3: off, ...
    sfreq : float | 150.15374755859375 Hz
    acq_pars : NoneType
    acq_stim : NoneType
    ctf_head_t : NoneType
    description : NoneType
    dev_ctf_t : NoneType
    experimenter : NoneType
    gantry_angle : NoneType
    hpi_subsystem : NoneType
    kit_system_id : NoneType
    line_freq : NoneType
    proj_id : NoneType
    proj_name : NoneType
    subject_info : NoneType
    xplotter_layout : NoneType
&gt;
</pre></div>
</div>
<p><a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> objects also have several built-in plotting methods;
here we show the power spectral density (PSD) for each sensor type with
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw.plot_psd" title="mne.io.Raw.plot_psd"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_psd()</span></code></a>, as well as a plot of the raw sensor traces with
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw.plot" title="mne.io.Raw.plot"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot()</span></code></a>. In the PSD plot, weâll only plot frequencies below
50 Hz (since our data are low-pass filtered at 40 Hz). In interactive Python
sessions, <a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw.plot" title="mne.io.Raw.plot"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot()</span></code></a> is interactive and allows scrolling,
scaling, bad channel marking, annotation, projector toggling, etc.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">raw</span><span class="o">.</span><span class="n">plot_psd</span><span class="p">(</span><span class="n">fmax</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">raw</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">duration</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_channels</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_001.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_001.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_002.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_002.png" />
</li>
</ul>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Effective window size : 13.639 (s)
Effective window size : 13.639 (s)
Effective window size : 13.639 (s)
</pre></div>
</div>
</div>
<div class="section" id="preprocessing">
<h2><a class="toc-backref" href="#id2">Preprocessing</a><a class="headerlink" href="#preprocessing" title="Permalink to this headline">Â¶</a></h2>
<p>MNE-Python supports a variety of preprocessing approaches and techniques
(maxwell filtering, signal-space projection, independent components analysis,
filtering, downsampling, etc); see the full list of capabilities in the
<a class="reference internal" href="../../python_reference.html#module-mne.preprocessing" title="mne.preprocessing"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mne.preprocessing</span></code></a> and <a class="reference internal" href="../../python_reference.html#module-mne.filter" title="mne.filter"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mne.filter</span></code></a> submodules. Here weâll clean
up our data by performing independent components analysis
(<a class="reference internal" href="../../generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA" title="mne.preprocessing.ICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">ICA</span></code></a>); for brevity weâll skip the steps that
helped us determined which components best capture the artifacts (see
<a class="reference internal" href="../preprocessing/plot_artifacts_correction_ica.html"><span class="doc">Artifact Correction with ICA</span></a> for a detailed
walk-through of that process).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># set up and fit the ICA</span>
<span class="n">ica</span> <span class="o">=</span> <a href="../../generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA" title="View documentation for mne.preprocessing.ICA"><span class="n">mne</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">ICA</span></a><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">97</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">800</span><span class="p">)</span>
<span class="n">ica</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
<span class="n">ica</span><span class="o">.</span><span class="n">exclude</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>  <span class="c1"># details on how we picked these are omitted here</span>
<span class="n">ica</span><span class="o">.</span><span class="n">plot_properties</span><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">picks</span><span class="o">=</span><span class="n">ica</span><span class="o">.</span><span class="n">exclude</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_003.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_003.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_004.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_004.png" />
</li>
</ul>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Fitting ICA to data using 364 channels (please be patient, this may take a while)
Inferring max_pca_components from picks
Selection by number: 20 components
Fitting ICA took 3.2s.
    Using multitaper spectrum estimation with 7 DPSS windows
</pre></div>
</div>
<p>Once weâre confident about which component(s) we want to remove, we pass them
as the <code class="docutils literal notranslate"><span class="pre">exclude</span></code> parameter and then apply the ICA to the raw signal. The
<a class="reference internal" href="../../generated/mne.preprocessing.ICA.html#mne.preprocessing.ICA.apply" title="mne.preprocessing.ICA.apply"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code></a> method requires the raw data to be
loaded into memory (by default itâs only read from disk as-needed), so weâll
use <a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw.load_data" title="mne.io.Raw.load_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_data()</span></code></a> first. Weâll also make a copy of the
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> object so we can compare the signal before and after
artifact removal side-by-side:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">orig_raw</span> <span class="o">=</span> <span class="n">raw</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">raw</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">ica</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>

<span class="c1"># show some frontal channels to clearly illustrate the artifact removal</span>
<span class="n">chs</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;MEG 0111&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0121&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0131&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0211&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0221&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0231&#39;</span><span class="p">,</span>
       <span class="s1">&#39;MEG 0311&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0321&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 0331&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 1511&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 1521&#39;</span><span class="p">,</span> <span class="s1">&#39;MEG 1531&#39;</span><span class="p">,</span>
       <span class="s1">&#39;EEG 001&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 002&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 003&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 004&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 005&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 006&#39;</span><span class="p">,</span>
       <span class="s1">&#39;EEG 007&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 008&#39;</span><span class="p">]</span>
<span class="n">chan_idxs</span> <span class="o">=</span> <span class="p">[</span><span class="n">raw</span><span class="o">.</span><span class="n">ch_names</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">ch</span><span class="p">)</span> <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">chs</span><span class="p">]</span>
<span class="n">orig_raw</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="n">chan_idxs</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">raw</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">order</span><span class="o">=</span><span class="n">chan_idxs</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_005.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_005.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_006.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_006.png" />
</li>
</ul>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Reading 0 ... 41699  =      0.000 ...   277.709 secs...
Transforming to ICA space (20 components)
Zeroing out 2 ICA components
</pre></div>
</div>
</div>
<div class="section" id="detecting-experimental-events">
<h2><a class="toc-backref" href="#id3">Detecting experimental events</a><a class="headerlink" href="#detecting-experimental-events" title="Permalink to this headline">Â¶</a></h2>
<p>The sample dataset includes several <a class="reference internal" href="../../glossary.html#term-stim-channel"><span class="xref std std-term">âSTIMâ channels</span></a>
that recorded electrical
signals sent from the stimulus delivery computer (as brief DC shifts /
squarewave pulses). These pulses (often called âtriggersâ) are used in this
dataset to mark experimental events: stimulus onset, stimulus type, and
participant response (button press). The individual STIM channels are
combined onto a single channel, in such a way that voltage
levels on that channel can be unambiguously decoded as a particular event
type. On older Neuromag systems (such as that used to record the sample data)
this summation channel was called <code class="docutils literal notranslate"><span class="pre">STI</span> <span class="pre">014</span></code>, so we can pass that channel
name to the <a class="reference internal" href="../../generated/mne.find_events.html#mne.find_events" title="mne.find_events"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.find_events()</span></code></a> function to recover the timing and
identity of the stimulus events.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">events</span> <span class="o">=</span> <a href="../../generated/mne.find_events.html#mne.find_events" title="View documentation for mne.find_events"><span class="n">mne</span><span class="o">.</span><span class="n">find_events</span></a><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">stim_channel</span><span class="o">=</span><span class="s1">&#39;STI 014&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">events</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># show the first 5</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>319 events found
Event IDs: [ 1  2  3  4  5 32]
[[6994    0    2]
 [7086    0    3]
 [7192    0    1]
 [7304    0    4]
 [7413    0    2]]
</pre></div>
</div>
<p>The resulting events array is an ordinary 3-column <a class="reference external" href="https://www.numpy.org/devdocs/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.18.dev0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NumPy</span> <span class="pre">array</span></code></a>, with sample number in the first column and integer event ID
in the last column; the middle column is usually ignored. Rather than keeping
track of integer event IDs, we can provide an <em>event dictionary</em> that maps
the integer IDs to experimental conditions or events. In this dataset, the
mapping looks like this:</p>
<table class="docutils align-default" id="sample-data-event-dict-table">
<colgroup>
<col style="width: 15%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Event ID</p></th>
<th class="head"><p>Condition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>auditory stimulus (tone) to the left ear</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>auditory stimulus (tone) to the right ear</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>visual stimulus (checkerboard) to the left visual field</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>visual stimulus (checkerboard) to the right visual field</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>smiley face (catch trial)</p></td>
</tr>
<tr class="row-odd"><td><p>32</p></td>
<td><p>subject button press</p></td>
</tr>
</tbody>
</table>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">event_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;auditory/left&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;auditory/right&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;visual/left&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
              <span class="s1">&#39;visual/right&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;smiley&#39;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;buttonpress&#39;</span><span class="p">:</span> <span class="mi">32</span><span class="p">}</span>
</pre></div>
</div>
<p>Event dictionaries like this one are used when extracting epochs from
continuous data; the <code class="docutils literal notranslate"><span class="pre">/</span></code> character in the dictionary keys allows pooling
across conditions by requesting partial condition descriptors (i.e.,
requesting <code class="docutils literal notranslate"><span class="pre">'auditory'</span></code> will select all epochs with Event IDs 1 and 2;
requesting <code class="docutils literal notranslate"><span class="pre">'left'</span></code> will select all epochs with Event IDs 1 and 3). An
example of this is shown in the next section. There is also a convenient
<a class="reference internal" href="../../generated/mne.viz.plot_events.html#mne.viz.plot_events" title="mne.viz.plot_events"><code class="xref py py-func docutils literal notranslate"><span class="pre">plot_events()</span></code></a> function for visualizing the distribution of
events across the duration of the recording (to make sure event detection
worked as expected). Here weâll also make use of the <a class="reference internal" href="../../generated/mne.Info.html#mne.Info" title="mne.Info"><code class="xref py py-class docutils literal notranslate"><span class="pre">Info</span></code></a>
attribute to get the sampling frequency of the recording (so our x-axis will
be in seconds instead of in samples).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <a href="../../generated/mne.viz.plot_events.html#mne.viz.plot_events" title="View documentation for mne.viz.plot_events"><span class="n">mne</span><span class="o">.</span><span class="n">viz</span><span class="o">.</span><span class="n">plot_events</span></a><span class="p">(</span><span class="n">events</span><span class="p">,</span> <span class="n">event_id</span><span class="o">=</span><span class="n">event_dict</span><span class="p">,</span> <span class="n">sfreq</span><span class="o">=</span><span class="n">raw</span><span class="o">.</span><span class="n">info</span><span class="p">[</span><span class="s1">&#39;sfreq&#39;</span><span class="p">])</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">right</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>  <span class="c1"># make room for the legend</span>
</pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_introduction_007.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_introduction_007.png" />
<p>For paradigms that are not event-related (e.g., analysis of resting-state
data), you can extract regularly spaced (possibly overlapping) spans of data
by creating events using <a class="reference internal" href="../../generated/mne.make_fixed_length_events.html#mne.make_fixed_length_events" title="mne.make_fixed_length_events"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.make_fixed_length_events()</span></code></a> and then
proceeding with epoching as described in the next section.</p>
</div>
<div class="section" id="epoching-continuous-data">
<h2><a class="toc-backref" href="#id4">Epoching continuous data</a><a class="headerlink" href="#epoching-continuous-data" title="Permalink to this headline">Â¶</a></h2>
<p>The <a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> object and the events array are the bare minimum
needed to create an <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a> object, which we create with the
<a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">mne.Epochs</span></code></a> class constructor. Here weâll also specify some data
quality constraints: weâll reject any epoch where peak-to-peak signal
amplitude is beyond reasonable limits for that channel type. This is done
with a <em>rejection dictionary</em>; you may include or omit thresholds for any of
the channel types present in your data. The values given here are reasonable
for this particular dataset, but may need to be adapted for different
hardware or recording conditions. For a more automated approach, consider
using the <a class="reference external" href="http://autoreject.github.io/">autoreject package</a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">reject_criteria</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">mag</span><span class="o">=</span><span class="mf">4000e-15</span><span class="p">,</span>     <span class="c1"># 4000 fT</span>
                       <span class="n">grad</span><span class="o">=</span><span class="mf">4000e-13</span><span class="p">,</span>    <span class="c1"># 4000 fT/cm</span>
                       <span class="n">eeg</span><span class="o">=</span><span class="mf">150e-6</span><span class="p">,</span>       <span class="c1"># 150 Î¼V</span>
                       <span class="n">eog</span><span class="o">=</span><span class="mf">250e-6</span><span class="p">)</span>       <span class="c1"># 250 Î¼V</span>
</pre></div>
</div>
<p>Weâll also pass the event dictionary as the <code class="docutils literal notranslate"><span class="pre">event_id</span></code> parameter (so we can
work with easy-to-pool event labels instead of the integer event IDs), and
specify <code class="docutils literal notranslate"><span class="pre">tmin</span></code> and <code class="docutils literal notranslate"><span class="pre">tmax</span></code> (the time relative to each event at which to
start and end each epoch). As mentioned above, by default
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> and <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a> data arenât loaded into memory
(theyâre accessed from disk only when needed), but here weâll force loading
into memory using the <code class="docutils literal notranslate"><span class="pre">preload=True</span></code> parameter so that we can see the
results of the rejection criteria being applied:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <a href="../../generated/mne.Epochs.html#mne.Epochs" title="View documentation for mne.Epochs"><span class="n">mne</span><span class="o">.</span><span class="n">Epochs</span></a><span class="p">(</span><span class="n">raw</span><span class="p">,</span> <span class="n">events</span><span class="p">,</span> <span class="n">event_id</span><span class="o">=</span><span class="n">event_dict</span><span class="p">,</span> <span class="n">tmin</span><span class="o">=-</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">tmax</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                    <span class="n">reject</span><span class="o">=</span><span class="n">reject_criteria</span><span class="p">,</span> <span class="n">preload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>319 matching events found
Applying baseline correction (mode: mean)
Not setting metadata
Created an SSP operator (subspace dimension = 4)
4 projection items activated
Loading data for 319 events and 106 original time points ...
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
    Rejecting  epoch based on MAG : [&#39;MEG 1711&#39;]
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
    Rejecting  epoch based on MAG : [&#39;MEG 1711&#39;]
    Rejecting  epoch based on EEG : [&#39;EEG 008&#39;]
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
    Rejecting  epoch based on EOG : [&#39;EOG 061&#39;]
10 bad epochs dropped
</pre></div>
</div>
<p>Next weâll pool across left/right stimulus presentations so we can compare
auditory versus visual responses. To avoid biasing our signals to the
left or right, weâll use <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs.equalize_event_counts" title="mne.Epochs.equalize_event_counts"><code class="xref py py-meth docutils literal notranslate"><span class="pre">equalize_event_counts()</span></code></a> first to
randomly sample epochs from each condition to match the number of epochs
present in the condition with the fewest good epochs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conds_we_care_about</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;auditory/left&#39;</span><span class="p">,</span> <span class="s1">&#39;auditory/right&#39;</span><span class="p">,</span>
                       <span class="s1">&#39;visual/left&#39;</span><span class="p">,</span> <span class="s1">&#39;visual/right&#39;</span><span class="p">]</span>
<span class="n">epochs</span><span class="o">.</span><span class="n">equalize_event_counts</span><span class="p">(</span><span class="n">conds_we_care_about</span><span class="p">)</span>  <span class="c1"># this operates in-place</span>
<span class="n">aud_epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">[</span><span class="s1">&#39;auditory&#39;</span><span class="p">]</span>
<span class="n">vis_epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">[</span><span class="s1">&#39;visual&#39;</span><span class="p">]</span>
<span class="k">del</span> <span class="n">raw</span><span class="p">,</span> <span class="n">epochs</span>  <span class="c1"># free up memory</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Dropped 7 epochs
</pre></div>
</div>
<p>Like <a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> objects, <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a> objects also have a
number of built-in plotting methods. One is <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs.plot_image" title="mne.Epochs.plot_image"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_image()</span></code></a>,
which shows each epoch as one row of an image map, with color representing
signal magnitude; the average evoked response and the sensor location are
shown below the image:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aud_epochs</span><span class="o">.</span><span class="n">plot_image</span><span class="p">(</span><span class="n">picks</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;MEG 1332&#39;</span><span class="p">,</span> <span class="s1">&#39;EEG 021&#39;</span><span class="p">])</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_008.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_008.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_009.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_009.png" />
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Both <a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> and <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a> objects have
<a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs.get_data" title="mne.Epochs.get_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_data()</span></code></a> methods that return the underlying data
as a <a class="reference external" href="https://www.numpy.org/devdocs/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.18.dev0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">NumPy</span> <span class="pre">array</span></code></a>. Both methods have a <code class="docutils literal notranslate"><span class="pre">picks</span></code>
parameter for subselecting which channel(s) to return; <code class="docutils literal notranslate"><span class="pre">raw.get_data()</span></code>
has additional parameters for restricting the time domain. The resulting
matrices have dimension <code class="docutils literal notranslate"><span class="pre">(n_channels,</span> <span class="pre">n_times)</span></code> for
<a class="reference internal" href="../../generated/mne.io.Raw.html#mne.io.Raw" title="mne.io.Raw"><code class="xref py py-class docutils literal notranslate"><span class="pre">Raw</span></code></a> and <code class="docutils literal notranslate"><span class="pre">(n_epochs,</span> <span class="pre">n_channels,</span> <span class="pre">n_times)</span></code> for
<a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a>.</p>
</div>
</div>
<div class="section" id="time-frequency-analysis">
<h2><a class="toc-backref" href="#id5">Time-frequency analysis</a><a class="headerlink" href="#time-frequency-analysis" title="Permalink to this headline">Â¶</a></h2>
<p>The <a class="reference internal" href="../../python_reference.html#module-mne.time_frequency" title="mne.time_frequency"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mne.time_frequency</span></code></a> submodule provides implementations of several
algorithms to compute time-frequency representations, power spectral density,
and cross-spectral density. Here, for example, weâll compute for the auditory
epochs the induced power at different frequencies and times, using Morlet
wavelets. On this dataset the result is not especially informative (it just
shows the evoked âauditory N100â response); see <a class="reference internal" href="../time-freq/plot_sensors_time_frequency.html#inter-trial-coherence"><span class="std std-ref">here</span></a> for a more extended example on a dataset with richer
frequency content.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">frequencies</span> <span class="o">=</span> <a href="https://www.numpy.org/devdocs/reference/generated/numpy.arange.html#numpy.arange" title="View documentation for numpy.arange"><span class="n">np</span><span class="o">.</span><span class="n">arange</span></a><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">power</span> <span class="o">=</span> <a href="../../generated/mne.time_frequency.tfr_morlet.html#mne.time_frequency.tfr_morlet" title="View documentation for mne.time_frequency.tfr_morlet"><span class="n">mne</span><span class="o">.</span><span class="n">time_frequency</span><span class="o">.</span><span class="n">tfr_morlet</span></a><span class="p">(</span><span class="n">aud_epochs</span><span class="p">,</span> <span class="n">n_cycles</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_itc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                      <span class="n">freqs</span><span class="o">=</span><span class="n">frequencies</span><span class="p">,</span> <span class="n">decim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">power</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="s1">&#39;MEG 1332&#39;</span><span class="p">])</span>
</pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_introduction_010.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_introduction_010.png" />
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>No baseline correction applied
</pre></div>
</div>
</div>
<div class="section" id="estimating-evoked-responses">
<h2><a class="toc-backref" href="#id6">Estimating evoked responses</a><a class="headerlink" href="#estimating-evoked-responses" title="Permalink to this headline">Â¶</a></h2>
<p>Now that we have our conditions in <code class="docutils literal notranslate"><span class="pre">aud_epochs</span></code> and <code class="docutils literal notranslate"><span class="pre">vis_epochs</span></code>, we can
get an estimate of evoked responses to auditory versus visual stimuli by
averaging together the epochs in each condition. This is as simple as calling
the <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs.average" title="mne.Epochs.average"><code class="xref py py-meth docutils literal notranslate"><span class="pre">average()</span></code></a> method on the <a class="reference internal" href="../../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-class docutils literal notranslate"><span class="pre">Epochs</span></code></a> object,
and then using a function from the <a class="reference internal" href="../../python_reference.html#module-mne.viz" title="mne.viz"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mne.viz</span></code></a> module to compare the
global field power for each sensor type of the two <a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked" title="mne.Evoked"><code class="xref py py-class docutils literal notranslate"><span class="pre">Evoked</span></code></a>
objects:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aud_evoked</span> <span class="o">=</span> <span class="n">aud_epochs</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>
<span class="n">vis_evoked</span> <span class="o">=</span> <span class="n">vis_epochs</span><span class="o">.</span><span class="n">average</span><span class="p">()</span>

<a href="../../generated/mne.viz.plot_compare_evokeds.html#mne.viz.plot_compare_evokeds" title="View documentation for mne.viz.plot_compare_evokeds"><span class="n">mne</span><span class="o">.</span><span class="n">viz</span><span class="o">.</span><span class="n">plot_compare_evokeds</span></a><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">auditory</span><span class="o">=</span><span class="n">aud_evoked</span><span class="p">,</span> <span class="n">visual</span><span class="o">=</span><span class="n">vis_evoked</span><span class="p">),</span>
                             <span class="n">show_legend</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">,</span>
                             <span class="n">show_sensors</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_011.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_011.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_012.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_012.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_013.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_013.png" />
</li>
</ul>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>No picks, plotting the GFP ...
Multiple channel types selected, returning one figure per type.
Combining all planar gradiometers with RMSE.
</pre></div>
</div>
<p>We can also get a more detailed view of each <a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked" title="mne.Evoked"><code class="xref py py-class docutils literal notranslate"><span class="pre">Evoked</span></code></a> object
using other plotting methods such as <a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked.plot_joint" title="mne.Evoked.plot_joint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_joint()</span></code></a> or
<a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked.plot_topomap" title="mne.Evoked.plot_topomap"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_topomap()</span></code></a>. Here weâll examine just the EEG channels,
and see the classic auditory evoked N100-P200 pattern over dorso-frontal
electrodes, then plot scalp topographies at some additional arbitrary times:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># sphinx_gallery_thumbnail_number = 13</span>
<span class="n">aud_evoked</span><span class="o">.</span><span class="n">plot_joint</span><span class="p">(</span><span class="n">picks</span><span class="o">=</span><span class="s1">&#39;eeg&#39;</span><span class="p">)</span>
<span class="n">aud_evoked</span><span class="o">.</span><span class="n">plot_topomap</span><span class="p">(</span><span class="n">times</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.08</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.12</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="n">ch_type</span><span class="o">=</span><span class="s1">&#39;eeg&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_014.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_014.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_015.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_015.png" />
</li>
</ul>
<p>Evoked objects can also be combined to show contrasts between conditions,
using the <a class="reference internal" href="../../generated/mne.combine_evoked.html#mne.combine_evoked" title="mne.combine_evoked"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.combine_evoked()</span></code></a> function. A simple difference can be
generated by negating one of the <a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked" title="mne.Evoked"><code class="xref py py-class docutils literal notranslate"><span class="pre">Evoked</span></code></a> objects passed into the
function. Weâll then plot the difference wave at each sensor using
<a class="reference internal" href="../../generated/mne.Evoked.html#mne.Evoked.plot_topo" title="mne.Evoked.plot_topo"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_topo()</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">evoked_diff</span> <span class="o">=</span> <a href="../../generated/mne.combine_evoked.html#mne.combine_evoked" title="View documentation for mne.combine_evoked"><span class="n">mne</span><span class="o">.</span><span class="n">combine_evoked</span></a><span class="p">([</span><span class="n">aud_evoked</span><span class="p">,</span> <span class="o">-</span><span class="n">vis_evoked</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;equal&#39;</span><span class="p">)</span>
<span class="n">evoked_diff</span><span class="o">.</span><span class="n">pick_types</span><span class="p">(</span><span class="s1">&#39;mag&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">plot_topo</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../../_images/sphx_glr_plot_introduction_016.png" class="sphx-glr-single-img" src="../../_images/sphx_glr_plot_introduction_016.png" />
</div>
<div class="section" id="inverse-modeling">
<h2><a class="toc-backref" href="#id7">Inverse modeling</a><a class="headerlink" href="#inverse-modeling" title="Permalink to this headline">Â¶</a></h2>
<p>Finally, we can estimate the origins of the evoked activity by projecting the
sensor data into this subjectâs <a class="reference internal" href="../../glossary.html#term-source-space"><span class="xref std std-term">source space</span></a> (a set of points either
on the cortical surface or within the cortical volume of that subject, as
estimated by structural MRI scans). MNE-Python supports lots of ways of doing
this (dynamic statistical parametric mapping, dipole fitting, beamformers,
etc.); here weâll use minimum-norm estimation (MNE) to generate a continuous
map of activation constrained to the cortical surface. MNE uses a linear
<a class="reference internal" href="../../glossary.html#term-inverse-operator"><span class="xref std std-term">inverse operator</span></a> to project EEG+MEG sensor measurements into the
source space. The inverse operator is computed from the
<a class="reference internal" href="../../glossary.html#term-forward-solution"><span class="xref std std-term">forward solution</span></a> for this subject and an estimate of <a class="reference internal" href="../source-modeling/plot_compute_covariance.html#tut-compute-covariance"><span class="std std-ref">the
covariance of sensor measurements</span></a>. For this
tutorial weâll skip those computational steps and load a pre-computed inverse
operator from disk (itâs included with the <a class="reference internal" href="../../manual/datasets_index.html#sample-dataset"><span class="std std-ref">sample data</span></a>). Because this âinverse problemâ is underdetermined (there
is no unique solution), here we further constrain the solution by providing a
regularization parameter specifying the relative smoothness of the current
estimates in terms of a signal-to-noise ratio (where ânoiseâ here is akin to
baseline activity level across all of cortex).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load inverse operator</span>
<span class="n">inverse_operator_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sample_data_folder</span><span class="p">,</span> <span class="s1">&#39;MEG&#39;</span><span class="p">,</span> <span class="s1">&#39;sample&#39;</span><span class="p">,</span>
                                     <span class="s1">&#39;sample_audvis-meg-oct-6-meg-inv.fif&#39;</span><span class="p">)</span>
<span class="n">inv_operator</span> <span class="o">=</span> <a href="../../generated/mne.minimum_norm.read_inverse_operator.html#mne.minimum_norm.read_inverse_operator" title="View documentation for mne.minimum_norm.read_inverse_operator"><span class="n">mne</span><span class="o">.</span><span class="n">minimum_norm</span><span class="o">.</span><span class="n">read_inverse_operator</span></a><span class="p">(</span><span class="n">inverse_operator_file</span><span class="p">)</span>
<span class="c1"># set signal-to-noise ratio (SNR) to compute regularization parameter (Î»Â²)</span>
<span class="n">snr</span> <span class="o">=</span> <span class="mf">3.</span>
<span class="n">lambda2</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">snr</span> <span class="o">**</span> <span class="mi">2</span>
<span class="c1"># generate the source time course (STC)</span>
<span class="n">stc</span> <span class="o">=</span> <a href="../../generated/mne.minimum_norm.apply_inverse.html#mne.minimum_norm.apply_inverse" title="View documentation for mne.minimum_norm.apply_inverse"><span class="n">mne</span><span class="o">.</span><span class="n">minimum_norm</span><span class="o">.</span><span class="n">apply_inverse</span></a><span class="p">(</span><span class="n">vis_evoked</span><span class="p">,</span> <span class="n">inv_operator</span><span class="p">,</span>
                                     <span class="n">lambda2</span><span class="o">=</span><span class="n">lambda2</span><span class="p">,</span>
                                     <span class="n">method</span><span class="o">=</span><span class="s1">&#39;MNE&#39;</span><span class="p">)</span>  <span class="c1"># or dSPM, sLORETA, eLORETA</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Reading inverse operator decomposition from /home/circleci/mne_data/MNE-sample-data/MEG/sample/sample_audvis-meg-oct-6-meg-inv.fif...
    Reading inverse operator info...
    [done]
    Reading inverse operator decomposition...
    [done]
    305 x 305 full covariance (kind = 1) found.
    Read a total of 4 projection items:
        PCA-v1 (1 x 102) active
        PCA-v2 (1 x 102) active
        PCA-v3 (1 x 102) active
        Average EEG reference (1 x 60) active
    Noise covariance matrix read.
    22494 x 22494 diagonal covariance (kind = 2) found.
    Source covariance matrix read.
    22494 x 22494 diagonal covariance (kind = 6) found.
    Orientation priors read.
    22494 x 22494 diagonal covariance (kind = 5) found.
    Depth priors read.
    Did not find the desired covariance matrix (kind = 3)
    Reading a source space...
    Computing patch statistics...
    Patch information added...
    Distance information added...
    [done]
    Reading a source space...
    Computing patch statistics...
    Patch information added...
    Distance information added...
    [done]
    2 source spaces read
    Read a total of 4 projection items:
        PCA-v1 (1 x 102) active
        PCA-v2 (1 x 102) active
        PCA-v3 (1 x 102) active
        Average EEG reference (1 x 60) active
    Source spaces transformed to the inverse solution coordinate frame
Preparing the inverse operator for use...
    Scaled noise and source covariance from nave = 1 to nave = 136
    Created the regularized inverter
    Created an SSP operator (subspace dimension = 3)
    Created the whitener using a noise covariance matrix with rank 302 (3 small eigenvalues omitted)
Applying inverse operator to &quot;0.50 * visual/left + 0.50 * visual/right&quot;...
    Picked 305 channels from the data
    Computing inverse...
    Eigenleads need to be weighted ...
    Computing residual...
    Explained  70.2% variance
    Combining the current components...
[done]
</pre></div>
</div>
<p>Finally, in order to plot the source estimate on the subjectâs cortical
surface weâll also need the path to the sample subjectâs structural MRI files
(the <code class="docutils literal notranslate"><span class="pre">subjects_dir</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># path to subjects&#39; MRI files</span>
<span class="n">subjects_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sample_data_folder</span><span class="p">,</span> <span class="s1">&#39;subjects&#39;</span><span class="p">)</span>
<span class="c1"># plot</span>
<span class="n">stc</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">initial_time</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hemi</span><span class="o">=</span><span class="s1">&#39;split&#39;</span><span class="p">,</span> <span class="n">views</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lat&#39;</span><span class="p">,</span> <span class="s1">&#39;med&#39;</span><span class="p">],</span>
         <span class="n">subjects_dir</span><span class="o">=</span><span class="n">subjects_dir</span><span class="p">)</span>
</pre></div>
</div>
<ul class="sphx-glr-horizontal">
<li><img alt="../../_images/sphx_glr_plot_introduction_017.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_017.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_018.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_018.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_019.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_019.png" />
</li>
<li><img alt="../../_images/sphx_glr_plot_introduction_020.png" class="sphx-glr-multi-img" src="../../_images/sphx_glr_plot_introduction_020.png" />
</li>
</ul>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Using control points [8.61922423e-11 1.06837855e-10 4.49139511e-10]
</pre></div>
</div>
<p>The remaining tutorials have <em>much more detail</em> on each of these topics (as
well as many other capabilities of MNE-Python not mentioned here:
connectivity analysis, encoding/decoding models, lots more visualization
options, etc). Read on to learn more!</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  42.066 seconds)</p>
<p><strong>Estimated memory usage:</strong>  373 MB</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-tutorials-intro-plot-introduction-py">
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/67f3916be608e2927a762d39e53acc5c/plot_introduction.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_introduction.py</span></code></a></p>
</div>
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/c92aa91c680730c756234cdbc466c558/plot_introduction.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_introduction.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


    </div>
    
  </div>
</div>
<footer class="footer">
  <div class="container institutions">
    <a href="https://www.massgeneral.org/"><img class="institution_lg" src="../../_static/institution_logos/MGH.svg" title="Massachusetts General Hospital" alt="Massachusetts General Hospital"/></a>
    <a href="https://martinos.org/"><img class="institution_lg" src="../../_static/institution_logos/Martinos.png" title="Athinoula A. Martinos Center for Biomedical Imaging" alt="Athinoula A. Martinos Center for Biomedical Imaging"/></a>
    <a href="https://hms.harvard.edu/"><img class="institution_lg" src="../../_static/institution_logos/Harvard.png" title="Harvard Medical School" alt="Harvard Medical School"/></a>
    <a href="https://web.mit.edu/"><img class="institution_sm" src="../../_static/institution_logos/MIT.svg" title="Massachusetts Institute of Technology" alt="Massachusetts Institute of Technology"/></a>
    <a href="https://www.nyu.edu/"><img class="institution_md" src="../../_static/institution_logos/NYU.png" title="New York University" alt="New York University"/></a>
    <a href="http://www.cea.fr/"><img class="institution_md" src="../../_static/institution_logos/CEA.png" title="Commissariat Ã  lÂ´Ã©nergie atomique et aux Ã©nergies alternatives" alt="Commissariat Ã  lÂ´Ã©nergie atomique et aux Ã©nergies alternatives"/></a>
    <a href="https://sci.aalto.fi/"><img class="institution_md" src="../../_static/institution_logos/Aalto.svg" title="Aalto-yliopiston perustieteiden korkeakoulu" alt="Aalto-yliopiston perustieteiden korkeakoulu"/></a>
    <a href="https://www.telecom-paris.fr/"><img class="institution_md" src="../../_static/institution_logos/Telecom_Paris_Tech.png" title="TÃ©lÃ©com ParisTech" alt="TÃ©lÃ©com ParisTech"/></a>
    <a href="https://www.washington.edu/"><img class="institution_sm" src="../../_static/institution_logos/Washington.png" title="University of Washington" alt="University of Washington"/></a>
    <a href="https://icm-institute.org/"><img class="institution_lg" src="../../_static/institution_logos/ICM.jpg" title="Institut du Cerveau et de la Moelle Ã©piniÃ¨re" alt="Institut du Cerveau et de la Moelle Ã©piniÃ¨re"/></a>
    <a href="https://www.bu.edu/"><img class="institution_sm" src="../../_static/institution_logos/BU.svg" title="Boston University" alt="Boston University"/></a>
    <a href="https://www.inserm.fr/"><img class="institution_xs" src="../../_static/institution_logos/Inserm.svg" title="Institut national de la santÃ© et de la recherche mÃ©dicale" alt="Institut national de la santÃ© et de la recherche mÃ©dicale"/></a>
    <a href="https://www.fz-juelich.de/"><img class="institution_sm" src="../../_static/institution_logos/Julich.svg" title="Forschungszentrum JÃ¼lich" alt="Forschungszentrum JÃ¼lich"/></a>
    <a href="https://www.tu-ilmenau.de/"><img class="institution_sm" src="../../_static/institution_logos/Ilmenau.gif" title="Technische UniversitÃ¤t Ilmenau" alt="Technische UniversitÃ¤t Ilmenau"/></a>
    <a href="https://bids.berkeley.edu/"><img class="institution_md" src="../../_static/institution_logos/BIDS.png" title="Berkeley Institute for Data Science" alt="Berkeley Institute for Data Science"/></a>
    <a href="https://www.inria.fr/"><img class="institution_sm" src="../../_static/institution_logos/inria.png" title="Institut national de recherche en informatique et en automatique" alt="Institut national de recherche en informatique et en automatique"/></a>
    <a href="https://www.au.dk/"><img class="institution_sm" src="../../_static/institution_logos/Aarhus.png" title="Aarhus Universitet" alt="Aarhus Universitet"/></a>
    <a href="https://www.uni-graz.at/"><img class="institution_md" src="../../_static/institution_logos/Graz.jpg" title="Karl-Franzens-UniversitÃ¤t Graz" alt="Karl-Franzens-UniversitÃ¤t Graz"/></a>
  </div>
  <div class="container">
    <ul class="list-inline">
      <li><a href="https://github.com/mne-tools/mne-python">GitHub</a></li>
      <li>Â·</li>
      <li><a href="https://mail.nmr.mgh.harvard.edu/mailman/listinfo/mne_analysis">Mailing list</a></li>
      <li>Â·</li>
      <li><a href="https://gitter.im/mne-tools/mne-python">Gitter</a></li>
      <li>Â·</li>
      <li><a href="whats_new.html">What's new</a></li>
      <li>Â·</li>
      <li><a href="faq.html#cite">Cite MNE</a></li>
      <li class="pull-right"><a href="#">Back to top</a></li>
    </ul>
    <p>&copy; Copyright 2012-2019, MNE Developers. Last updated on 2019-07-11.</p>
  </div>
</footer>
  </body>
</html>