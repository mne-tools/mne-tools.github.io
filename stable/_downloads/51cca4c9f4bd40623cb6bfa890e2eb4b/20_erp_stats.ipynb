{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Visualising statistical significance thresholds on EEG data\n\nMNE-Python provides a range of tools for statistical hypothesis testing\nand the visualisation of the results. Here, we show a few options for\nexploratory and confirmatory tests - e.g., targeted t-tests, cluster-based\npermutation approaches (here with Threshold-Free Cluster Enhancement);\nand how to visualise the results.\n\nThe underlying data comes from :footcite:`DufauEtAl2015`; we contrast long vs.\nshort words. TFCE is described in :footcite:`SmithNichols2009`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# License: BSD-3-Clause\n# Copyright the MNE-Python contributors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import ttest_ind\n\nimport mne\nfrom mne.channels import find_ch_adjacency, make_1020_channel_selections\nfrom mne.stats import spatio_temporal_cluster_test\n\nnp.random.seed(0)\n\n# Load the data\npath = mne.datasets.kiloword.data_path() / \"kword_metadata-epo.fif\"\nepochs = mne.read_epochs(path)\n# These data are quite smooth, so to speed up processing we'll (unsafely!) just\n# decimate them\nepochs.decimate(4, verbose=\"error\")\nname = \"NumberOfLetters\"\n\n# Split up the data by the median length in letters via the attached metadata\nmedian_value = str(epochs.metadata[name].median())\nlong_words = epochs[name + \" > \" + median_value]\nshort_words = epochs[name + \" < \" + median_value]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we have a specific point in space and time we wish to test, it can be\nconvenient to convert the data into Pandas Dataframe format. In this case,\nthe :class:`mne.Epochs` object has a convenient\n:meth:`mne.Epochs.to_data_frame` method, which returns a dataframe.\nThis dataframe can then be queried for specific time windows and sensors.\nThe extracted data can be submitted to standard statistical tests. Here,\nwe conduct t-tests on the difference between long and short words.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_windows = ((0.2, 0.25), (0.35, 0.45))\nelecs = [\"Fz\", \"Cz\", \"Pz\"]\nindex = [\"condition\", \"epoch\", \"time\"]\n\n# display the EEG data in Pandas format (first 5 rows)\nprint(epochs.to_data_frame(index=index)[elecs].head())\n\nreport = \"{elec}, time: {tmin}-{tmax} s; t({df})={t_val:.3f}, p={p:.3f}\"\nprint(\"\\nTargeted statistical test results:\")\nfor tmin, tmax in time_windows:\n    long_df = long_words.copy().crop(tmin, tmax).to_data_frame(index=index)\n    short_df = short_words.copy().crop(tmin, tmax).to_data_frame(index=index)\n    for elec in elecs:\n        # extract data\n        A = long_df[elec].groupby(\"condition\").mean()\n        B = short_df[elec].groupby(\"condition\").mean()\n\n        # conduct t test\n        t, p = ttest_ind(A, B)\n\n        # display results\n        format_dict = dict(\n            elec=elec, tmin=tmin, tmax=tmax, df=len(epochs.events) - 2, t_val=t, p=p\n        )\n        print(report.format(**format_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Absent specific hypotheses, we can also conduct an exploratory\nmass-univariate analysis at all sensors and time points. This requires\ncorrecting for multiple tests.\nMNE offers various methods for this; amongst them, cluster-based permutation\nmethods allow deriving power from the spatio-temoral correlation structure\nof the data. Here, we use TFCE.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate adjacency matrix between sensors from their locations\nadjacency, _ = find_ch_adjacency(epochs.info, \"eeg\")\n\n# Extract data: transpose because the cluster test requires channels to be last\n# In this case, inference is done over items. In the same manner, we could\n# also conduct the test over, e.g., subjects.\nX = [\n    long_words.get_data(copy=False).transpose(0, 2, 1),\n    short_words.get_data(copy=False).transpose(0, 2, 1),\n]\ntfce = dict(start=0.4, step=0.4)  # ideally start and step would be smaller\n\n# Calculate statistical thresholds\nt_obs, clusters, cluster_pv, h0 = spatio_temporal_cluster_test(\n    X, tfce, adjacency=adjacency, n_permutations=100\n)  # a more standard number would be 1000+\nsignificant_points = cluster_pv.reshape(t_obs.shape).T < 0.05\nprint(str(significant_points.sum()) + \" points selected by TFCE ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results of these mass univariate analyses can be visualised by plotting\n:class:`mne.Evoked` objects as images (via :class:`mne.Evoked.plot_image`)\nand masking points for significance.\nHere, we group channels by Regions of Interest to facilitate localising\neffects on the head.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We need an evoked object to plot the image to be masked\nevoked = mne.combine_evoked(\n    [long_words.average(), short_words.average()], weights=[1, -1]\n)  # calculate difference wave\ntime_unit = dict(time_unit=\"s\")\nevoked.plot_joint(\n    title=\"Long vs. short words\", ts_args=time_unit, topomap_args=time_unit\n)  # show difference wave\n\n# Create ROIs by checking channel labels\nselections = make_1020_channel_selections(evoked.info, midline=\"12z\")\n\n# Visualize the results\nfig, axes = plt.subplots(nrows=3, figsize=(8, 8))\naxes = {sel: ax for sel, ax in zip(selections, axes.ravel())}\nevoked.plot_image(\n    axes=axes,\n    group_by=selections,\n    colorbar=False,\n    show=False,\n    mask=significant_points,\n    show_names=\"all\",\n    titles=None,\n    **time_unit,\n)\nplt.colorbar(axes[\"Left\"].images[-1], ax=list(axes.values()), shrink=0.3, label=\"\u00b5V\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}