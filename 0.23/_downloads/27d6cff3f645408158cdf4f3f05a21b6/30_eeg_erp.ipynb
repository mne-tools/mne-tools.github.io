{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# EEG processing and Event Related Potentials (ERPs)\n\nThis tutorial shows how to perform standard ERP analyses in MNE-Python. Most of\nthe material here is covered in other tutorials too, but for convenience the\nfunctions and methods most useful for ERP analyses are collected here, with\nlinks to other tutorials where more detailed information is given.\n\nAs usual we'll start by importing the modules we need and loading some example\ndata. Instead of parsing the events from the raw data's :term:`stim channel`\n(like we do in `this tutorial <tut-events-vs-annotations>`), we'll load\nthe events from an external events file. Finally, to speed up computations so\nour documentation server can handle them, we'll crop the raw data from ~4.5\nminutes down to 90 seconds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport mne\n\nsample_data_folder = mne.datasets.sample.data_path()\nsample_data_raw_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n                                    'sample_audvis_filt-0-40_raw.fif')\nraw = mne.io.read_raw_fif(sample_data_raw_file, preload=False)\n\nsample_data_events_file = os.path.join(sample_data_folder, 'MEG', 'sample',\n                                       'sample_audvis_filt-0-40_raw-eve.fif')\nevents = mne.read_events(sample_data_events_file)\n\nraw.crop(tmax=90)  # in seconds; happens in-place\n# discard events >90 seconds (not strictly necessary: avoids some warnings)\nevents = events[events[:, 0] <= raw.last_samp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The file that we loaded has already been partially processed: 3D sensor\nlocations have been saved as part of the ``.fif`` file, the data have been\nlow-pass filtered at 40 Hz, and a common average reference is set for the\nEEG channels, stored as a projector (see `section-avg-ref-proj` in the\n`tut-set-eeg-ref` tutorial for more info about when you may want to do\nthis). We'll discuss how to do each of these below.\n\nSince this is a combined EEG+MEG dataset, let's start by restricting the data\nto just the EEG and EOG channels. This will cause the other projectors saved\nin the file (which apply only to magnetometer channels) to be removed. By\nlooking at the measurement info we can see that we now have 59 EEG channels\nand 1 EOG channel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.pick(['eeg', 'eog']).load_data()\nraw.info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Channel names and types\n\nIn practice it's quite common to have some channels labelled as EEG that are\nactually EOG channels. `~mne.io.Raw` objects have a\n`~mne.io.Raw.set_channel_types` method that you can use to change a channel\nthat is labeled as ``eeg`` into an ``eog`` type. You can also rename channels\nusing the `~mne.io.Raw.rename_channels` method. Detailed examples of both of\nthese methods can be found in the tutorial `tut-raw-class`. In this data\nthe channel types are all correct already, so for now we'll just rename the\nchannels to remove a space and a leading zero in the channel names, and\nconvert to lowercase:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "channel_renaming_dict = {name: name.replace(' 0', '').lower()\n                         for name in raw.ch_names}\n_ = raw.rename_channels(channel_renaming_dict)  # happens in-place"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Channel locations\n\nThe tutorial `tut-sensor-locations` describes MNE-Python's handling of\nsensor positions in great detail. To briefly summarize: MNE-Python\ndistinguishes :term:`montages <montage>` (which contain sensor positions in\n3D: ``x``, ``y``, ``z``, in meters) from :term:`layouts <layout>` (which\ndefine 2D arrangements of sensors for plotting approximate overhead diagrams\nof sensor positions). Additionally, montages may specify *idealized* sensor\npositions (based on, e.g., an idealized spherical headshape model) or they\nmay contain *realistic* sensor positions obtained by digitizing the 3D\nlocations of the sensors when placed on the actual subject's head.\n\nThis dataset has realistic digitized 3D sensor locations saved as part of the\n``.fif`` file, so we can view the sensor locations in 2D or 3D using the\n`~mne.io.Raw.plot_sensors` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.plot_sensors(show_names=True)\nfig = raw.plot_sensors('3d')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you're working with a standard montage like the `10-20 <ten_twenty_>`_\nsystem, you can add sensor locations to the data like this:\n``raw.set_montage('standard_1020')``.  See `tut-sensor-locations` for\ninfo on what other standard montages are built-in to MNE-Python.\n\nIf you have digitized realistic sensor locations, there are dedicated\nfunctions for loading those digitization files into MNE-Python; see\n`reading-dig-montages` for discussion and `dig-formats` for a list\nof supported formats. Once loaded, the digitized sensor locations can be\nadded to the data by passing the loaded montage object to\n``raw.set_montage()``.\n\n\n## Setting the EEG reference\n\nAs mentioned above, this data already has an EEG common average reference\nadded as a :term:`projector`. We can view the effect of this on the raw data\nby plotting with and without the projector applied:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for proj in (False, True):\n    fig = raw.plot(n_channels=5, proj=proj, scalings=dict(eeg=50e-6))\n    fig.subplots_adjust(top=0.9)  # make room for title\n    ref = 'Average' if proj else 'No'\n    fig.suptitle(f'{ref} reference', size='xx-large', weight='bold')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The referencing scheme can be changed with the function\n`mne.set_eeg_reference` (which by default operates on a *copy* of the data)\nor the `raw.set_eeg_reference() <mne.io.Raw.set_eeg_reference>` method (which\nalways modifies the data in-place). The tutorial `tut-set-eeg-ref` shows\nseveral examples of this.\n\n\n## Filtering\n\nMNE-Python has extensive support for different ways of filtering data. For a\ngeneral discussion of filter characteristics and MNE-Python defaults, see\n`disc-filtering`. For practical examples of how to apply filters to your\ndata, see `tut-filter-resample`. Here, we'll apply a simple high-pass\nfilter for illustration:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "raw.filter(l_freq=0.1, h_freq=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evoked responses: epoching and averaging\n\nThe general process for extracting evoked responses from continuous data is\nto use the `~mne.Epochs` constructor, and then average the resulting epochs\nto create an `~mne.Evoked` object. In MNE-Python, events are represented as\na :class:`NumPy array <numpy.ndarray>` of sample numbers and integer event\ncodes. The event codes are stored in the last column of the events array:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "np.unique(events[:, -1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `tut-event-arrays` tutorial discusses event arrays in more detail.\nInteger event codes are mapped to more descriptive text using a Python\n:class:`dictionary <dict>` usually called ``event_id``. This mapping is\ndetermined by your experiment code (i.e., it reflects which event codes you\nchose to use to represent different experimental events or conditions). For\nthe `sample-dataset` data has the following mapping:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "event_dict = {'auditory/left': 1, 'auditory/right': 2, 'visual/left': 3,\n              'visual/right': 4, 'face': 5, 'buttonpress': 32}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can extract epochs from the continuous data. An interactive plot\nallows you to click on epochs to mark them as \"bad\" and drop them from the\nanalysis (it is not interactive on the documentation website, but will be\nwhen you run `epochs.plot() <mne.Epochs.plot>` in a Python console).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs = mne.Epochs(raw, events, event_id=event_dict, tmin=-0.3, tmax=0.7,\n                    preload=True)\nfig = epochs.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to automatically drop epochs, when first creating them or\nlater on, by providing maximum peak-to-peak signal value thresholds (pass to\nthe `~mne.Epochs` constructor as the ``reject`` parameter; see\n`tut-reject-epochs-section` for details).  You can also do this after\nthe epochs are already created, using the `~mne.Epochs.drop_bad` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reject_criteria = dict(eeg=100e-6,  # 100 \u00b5V\n                       eog=200e-6)  # 200 \u00b5V\n_ = epochs.drop_bad(reject=reject_criteria)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we generate a barplot of which channels contributed most to epochs\ngetting rejected. If one channel is responsible for lots of epoch rejections,\nit may be worthwhile to mark that channel as \"bad\" in the `~mne.io.Raw`\nobject and then re-run epoching (fewer channels w/ more good epochs may be\npreferable to keeping all channels but losing many epochs). See\n`tut-bad-channels` for more info.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs.plot_drop_log()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Another way in which epochs can be automatically dropped is if the\n`~mne.io.Raw` object they're extracted from contains :term:`annotations` that\nbegin with either ``bad`` or ``edge`` (\"edge\" annotations are automatically\ninserted when concatenating two separate `~mne.io.Raw` objects together). See\n`tut-reject-data-spans` for more information about annotation-based\nepoch rejection.\n\nNow that we've dropped the bad epochs, let's look at our evoked responses for\nsome conditions we care about. Here the `~mne.Epochs.average` method will\ncreate and `~mne.Evoked` object, which we can then plot. Notice that we\\\nselect which condition we want to average using the square-bracket indexing\n(like a :class:`dictionary <dict>`); that returns a smaller epochs object\ncontaining just the epochs from that condition, to which we then apply the\n`~mne.Epochs.average` method:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l_aud = epochs['auditory/left'].average()\nl_vis = epochs['visual/left'].average()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These `~mne.Evoked` objects have their own interactive plotting method\n(though again, it won't be interactive on the documentation website):\nclick-dragging a span of time will generate a scalp field topography for that\ntime span. Here we also demonstrate built-in color-coding the channel traces\nby location:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig1 = l_aud.plot()\nfig2 = l_vis.plot(spatial_colors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scalp topographies can also be obtained non-interactively with the\n`~mne.Evoked.plot_topomap` method. Here we display topomaps of the average\nfield in 50 ms time windows centered at -200 ms, 100 ms, and 400 ms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l_aud.plot_topomap(times=[-0.2, 0.1, 0.4], average=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Considerable customization of these plots is possible, see the docstring of\n`~mne.Evoked.plot_topomap` for details.\n\nThere is also a built-in method for combining \"butterfly\" plots of the\nsignals with scalp topographies, called `~mne.Evoked.plot_joint`. Like\n`~mne.Evoked.plot_topomap` you can specify times for the scalp topographies\nor you can let the method choose times automatically, as is done here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l_aud.plot_joint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Global field power (GFP)\n\nGlobal field power :footcite:`Lehmann1980,Lehmann1984,Murray2008` is,\ngenerally speaking, a measure of agreement of the signals picked up by all\nsensors across the entire scalp: if all sensors have the same value at a\ngiven time point, the GFP will be zero at that time point; if the signals\ndiffer, the GFP will be non-zero at that time point. GFP\npeaks may reflect \"interesting\" brain activity, warranting further\ninvestigation. Mathematically, the GFP is the population standard\ndeviation across all sensors, calculated separately for every time point.\n\nYou can plot the GFP using `evoked.plot(gfp=True) <mne.Evoked.plot>`. The GFP\ntrace will be black if ``spatial_colors=True`` and green otherwise. The EEG\nreference does not affect the GFP:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for evk in (l_aud, l_vis):\n    evk.plot(gfp=True, spatial_colors=True, ylim=dict(eeg=[-12, 12]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To plot the GFP by itself you can pass ``gfp='only'`` (this makes it easier\nto read off the GFP data values, because the scale is aligned):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "l_aud.plot(gfp='only')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As stated above, the GFP is the population standard deviation of the signal\nacross channels. To compute it manually, we can leverage the fact that\n`evoked.data <mne.Evoked.data>` is a :class:`NumPy array <numpy.ndarray>`,\nand verify by plotting it using matplotlib commands:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gfp = l_aud.data.std(axis=0, ddof=0)\n\n# Reproducing the MNE-Python plot style seen above\nfig, ax = plt.subplots()\nax.plot(l_aud.times, gfp * 1e6, color='lime')\nax.fill_between(l_aud.times, gfp * 1e6, color='lime', alpha=0.2)\nax.set(xlabel='Time (s)', ylabel='GFP (\u00b5V)', title='EEG')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing regions of interest (ROIs): averaging across channels\n\nSince our sample data is responses to left and right auditory and visual\nstimuli, we may want to compare left versus right ROIs. To average across\nchannels in a region of interest, we first find the channel indices we want.\nLooking back at the 2D sensor plot above, we might choose the following for\nleft and right ROIs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "left = ['eeg17', 'eeg18', 'eeg25', 'eeg26']\nright = ['eeg23', 'eeg24', 'eeg34', 'eeg35']\n\nleft_ix = mne.pick_channels(l_aud.info['ch_names'], include=left)\nright_ix = mne.pick_channels(l_aud.info['ch_names'], include=right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can create a new Evoked with 2 virtual channels (one for each ROI):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "roi_dict = dict(left_ROI=left_ix, right_ROI=right_ix)\nroi_evoked = mne.channels.combine_channels(l_aud, roi_dict, method='mean')\nprint(roi_evoked.info['ch_names'])\nroi_evoked.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing conditions\n\nIf we wanted to compare our auditory and visual stimuli, a useful function is\n`mne.viz.plot_compare_evokeds`. By default this will combine all channels in\neach evoked object using global field power (or RMS for MEG channels); here\ninstead we specify to combine by averaging, and restrict it to a subset of\nchannels by passing ``picks``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evokeds = dict(auditory=l_aud, visual=l_vis)\npicks = [f'eeg{n}' for n in range(10, 15)]\nmne.viz.plot_compare_evokeds(evokeds, picks=picks, combine='mean')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also easily get confidence intervals by treating each epoch as a\nseparate observation using the `~mne.Epochs.iter_evoked` method. A confidence\ninterval across subjects could also be obtained, by passing a list of\n`~mne.Evoked` objects (one per subject) to the\n`~mne.viz.plot_compare_evokeds` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evokeds = dict(auditory=list(epochs['auditory/left'].iter_evoked()),\n               visual=list(epochs['visual/left'].iter_evoked()))\nmne.viz.plot_compare_evokeds(evokeds, combine='mean', picks=picks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also compare conditions by subtracting one `~mne.Evoked` object from\nanother using the `mne.combine_evoked` function (this function also allows\npooling of epochs without subtraction).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aud_minus_vis = mne.combine_evoked([l_aud, l_vis], weights=[1, -1])\naud_minus_vis.plot_joint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The code above yields an **equal-weighted difference**. If you have\n    imbalanced trial numbers, you might want to equalize the number of events\n    per condition first by using `epochs.equalize_event_counts()\n    <mne.Epochs.equalize_event_counts>` before averaging.</p></div>\n\n\n## Grand averages\n\nTo compute grand averages across conditions (or subjects), you can pass a\nlist of `~mne.Evoked` objects to `mne.grand_average`. The result is another\n`~mne.Evoked` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grand_average = mne.grand_average([l_aud, l_vis])\nprint(grand_average)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For combining *conditions* it is also possible to make use of :term:`HED`\ntags in the condition names when selecting which epochs to average. For\nexample, we have the condition names:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list(event_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can select the auditory conditions (left and right together) by passing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "epochs['auditory'].average()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "see `tut-section-subselect-epochs` for details.\n\nThe tutorials `tut-epochs-class` and `tut-evoked-class` have many\nmore details about working with the `~mne.Epochs` and `~mne.Evoked` classes.\n\n\n\n### References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}