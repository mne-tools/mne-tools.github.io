
<!DOCTYPE html>


<html lang="en" data-content_root="../" data-theme="auto">

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Datasets Overview &#8212; MNE 1.11.0.dev42+gd25726187 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "auto";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "auto";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css?v=4c2284e1" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=87036e28"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-5TBCPCRB6X"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-5TBCPCRB6X');
            </script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'documentation/datasets';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://mne.tools/dev/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'dev';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <script defer="defer" src="../_static/js/custom-icons.js?v=8bbd8100"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Command line tools using Python" href="../generated/commands.html" />
    <link rel="prev" title="Design philosophy" href="design_philosophy.html" />
    <link rel="canonical" href="https://mne.tools/stable/documentation/datasets.html" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="1.11" />

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="auto">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/mne_logo_small.svg" class="logo__image only-light" alt="MNE 1.11.0.dev42+gd25726187 documentation - Home"/>
    <img src="../_static/mne_logo_small.svg" class="logo__image only-dark pst-js-only" alt="MNE 1.11.0.dev42+gd25726187 documentation - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install/index.html">
    Install
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Documentation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/python_reference.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../help/index.html">
    Get Help
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../development/index.html">
    Development
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="External Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/rKfvxTuATa" title="Discord (office hours)" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord (office hours)</span></a>
        </li>
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="https://fosstodon.org/@mne" title="Mastodon" class="nav-link pst-navbar-icon" rel="me" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-mastodon fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Mastodon</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mne.discourse.group/" title="Q&A Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Q&A Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/mne-tools/mne-python" title="Code Repository" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Code Repository</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/sponsors/mne-tools" title="Sponsor us on GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-regular fa-heart fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Sponsor us on GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mne-python" title="Donate via OpenCollective" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-opencollective fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Donate via OpenCollective</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install/index.html">
    Install
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="index.html">
    Documentation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/python_reference.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../help/index.html">
    Get Help
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../development/index.html">
    Development
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="External Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://discord.gg/rKfvxTuATa" title="Discord (office hours)" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discord fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Discord (office hours)</span></a>
        </li>
        <li class="nav-item">
            
          
          
          
          
          
          
          
          
          
          <a href="https://fosstodon.org/@mne" title="Mastodon" class="nav-link pst-navbar-icon" rel="me" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-mastodon fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Mastodon</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://mne.discourse.group/" title="Q&A Forum" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-discourse fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Q&A Forum</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/mne-tools/mne-python" title="Code Repository" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Code Repository</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/sponsors/mne-tools" title="Sponsor us on GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-regular fa-heart fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Sponsor us on GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://opencollective.com/mne-python" title="Donate via OpenCollective" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-custom fa-opencollective fa-fw fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Donate via OpenCollective</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_tutorials/index.html">Tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/intro/index.html">Introductory tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/10_overview.html">Overview of MEG/EEG analysis with MNE-Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/15_inplace.html">Modifying data in-place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/20_events_from_raw.html">Parsing events from raw data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/30_info.html">The Info data structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/40_sensor_locations.html">Working with sensor locations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/50_configure_mne.html">Configuring MNE-Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/intro/70_report.html">Getting started with mne.Report</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/io/index.html">Reading data for different recording systems</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/io/10_reading_meg_data.html">Importing data from MEG devices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/io/20_reading_eeg_data.html">Importing data from EEG devices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/io/30_reading_fnirs_data.html">Importing data from fNIRS devices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/io/60_ctf_bst_auditory.html">Working with CTF data: the Brainstorm auditory dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/io/70_reading_eyetracking_data.html">Importing Data from Eyetracking devices</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/raw/index.html">Working with continuous data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/raw/10_raw_overview.html">The Raw data structure: continuous data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/raw/20_event_arrays.html">Working with events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/raw/30_annotate_raw.html">Annotating continuous data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/raw/40_visualize_raw.html">Built-in plotting methods for Raw objects</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/preprocessing/index.html">Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/10_preprocessing_overview.html">Overview of artifact detection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/15_handling_bad_channels.html">Handling bad channels</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/20_rejecting_bad_data.html">Rejecting bad data spans and breaks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/25_background_filtering.html">Background information on filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/30_filtering_resampling.html">Filtering and resampling data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/35_artifact_correction_regression.html">Repairing artifacts with regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/40_artifact_correction_ica.html">Repairing artifacts with ICA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/45_projectors_background.html">Background on projectors and projections</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/50_artifact_correction_ssp.html">Repairing artifacts with SSP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/55_setting_eeg_reference.html">Setting the EEG reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/59_head_positions.html">Extracting and visualizing subject head movement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/60_maxwell_filtering_sss.html">Signal-space separation (SSS) and Maxwell filtering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/70_fnirs_processing.html">Preprocessing functional near-infrared spectroscopy (fNIRS) data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/80_opm_processing.html">Preprocessing optically pumped magnetometer (OPM) MEG data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/preprocessing/90_eyetracking_data.html">Working with eye tracker data in MNE-Python</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/epochs/index.html">Segmenting continuous data into epochs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/10_epochs_overview.html">The Epochs data structure: discontinuous data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/15_baseline_regression.html">Regression-based baseline correction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/20_visualize_epochs.html">Visualizing epoched data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/30_epochs_metadata.html">Working with Epoch metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/40_autogenerate_metadata.html">Auto-generating Epochs metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/50_epochs_to_data_frame.html">Exporting Epochs to Pandas DataFrames</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/epochs/60_make_fixed_length_epochs.html">Divide continuous data into equally-spaced epochs</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/evoked/index.html">Estimating evoked responses</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/evoked/10_evoked_overview.html">The Evoked data structure: evoked/averaged data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/evoked/20_visualize_evoked.html">Visualizing Evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/evoked/30_eeg_erp.html">EEG analysis - Event-Related Potentials (ERPs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/evoked/40_whitened.html">Plotting whitened data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/time-freq/index.html">Time-frequency analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/time-freq/10_spectrum_class.html">The Spectrum and EpochsSpectrum classes: frequency-domain data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/time-freq/20_sensors_time_frequency.html">Frequency and time-frequency sensor analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/time-freq/50_ssvep.html">Frequency-tagging: Basic analysis of an SSVEP/vSSR dataset</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/forward/index.html">Forward models and source spaces</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/10_background_freesurfer.html">FreeSurfer MRI reconstruction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/20_source_alignment.html">Source alignment and coordinate frames</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/25_automated_coreg.html">Using an automated approach to coregistration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/30_forward.html">Head model and forward computation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/35_eeg_no_mri.html">EEG forward operator with a template MRI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/50_background_freesurfer_mne.html">How MNE uses FreeSurfer’s outputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/80_fix_bem_in_blender.html">Fixing BEM and head surfaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/forward/90_compute_covariance.html">Computing a covariance matrix</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/inverse/index.html">Source localization and inverses</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/10_stc_class.html">The SourceEstimate data structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/20_dipole_fit.html">Source localization with equivalent current dipole (ECD) fit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/30_mne_dspm_loreta.html">Source localization with MNE, dSPM, sLORETA, and eLORETA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/35_dipole_orientations.html">The role of dipole orientations in distributed source localization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/40_mne_fixed_free.html">Computing various MNE solutions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/50_beamformer_lcmv.html">Source reconstruction using an LCMV beamformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/60_visualize_stc.html">Visualize source time courses (stcs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/70_eeg_mri_coords.html">EEG source localization given electrode locations on an MRI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/80_brainstorm_phantom_elekta.html">Brainstorm Elekta phantom dataset tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/85_brainstorm_phantom_ctf.html">Brainstorm CTF phantom dataset tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/90_phantom_4DBTi.html">4D Neuroimaging/BTi phantom dataset tutorial</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/inverse/95_phantom_KIT.html">KIT phantom dataset tutorial</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/index.html">Statistical analysis of sensor data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/10_background_stats.html">Statistical inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/20_erp_stats.html">Visualising statistical significance thresholds on EEG data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/40_cluster_1samp_time_freq.html">Non-parametric 1 sample cluster statistic on single trial power</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/50_cluster_between_time_freq.html">Non-parametric between conditions cluster statistic on single trial power</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/70_cluster_rmANOVA_time_freq.html">Mass-univariate twoway repeated measures ANOVA on single trial power</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-sensor-space/75_cluster_ftest_spatiotemporal.html">Spatiotemporal permutation F-test on full sensor data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/stats-source-space/index.html">Statistical analysis of source estimates</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-source-space/20_cluster_1samp_spatiotemporal.html">Permutation t-test on source data with spatio-temporal clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-source-space/30_cluster_ftest_spatiotemporal.html">2 samples permutation test on source data with spatio-temporal clustering</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/stats-source-space/60_cluster_rmANOVA_spatiotemporal.html">Repeated measures ANOVA on source data with spatio-temporal clustering</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/machine-learning/index.html">Machine learning models of neural activity</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/machine-learning/30_strf.html">Spectro-temporal receptive field (STRF) estimation on continuous data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/machine-learning/50_decoding.html">Decoding (MVPA)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/clinical/index.html">Clinical applications</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/clinical/20_seeg.html">Working with sEEG data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/clinical/30_ecog.html">Working with ECoG data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/clinical/60_sleep.html">Sleep stage classification from polysomnography (PSG) data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/simulation/index.html">Simulation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/simulation/10_array_objs.html">Creating MNE-Python data structures from scratch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/simulation/70_point_spread.html">Corrupt known signal with point spread</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/simulation/80_dics.html">DICS for power mapping</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_tutorials/visualization/index.html">Visualization tutorials</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/visualization/10_publication_figure.html">Make figures more publication ready</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_tutorials/visualization/20_ui_events.html">Using the event system to link figures</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../auto_examples/index.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/io/index.html">Input/Output</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/io/elekta_epochs.html">Getting averaging info from .fif files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/io/read_impedances.html">Getting impedances from raw files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/io/read_neo_format.html">How to use data in neural ensemble (NEO) format</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/io/read_noise_covariance_matrix.html">Reading/Writing a noise covariance matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/io/read_xdf.html">Reading XDF EEG data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/simulation/index.html">Data Simulation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/simulation/plot_stc_metrics.html">Compare simulated and estimated source activity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/simulation/simulate_evoked_data.html">Generate simulated evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/simulation/simulate_raw_data.html">Generate simulated raw data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/simulation/simulated_raw_data_using_subject_anatomy.html">Simulate raw data using subject anatomy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/simulation/source_simulator.html">Generate simulated source data</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/preprocessing/index.html">Preprocessing</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/contralateral_referencing.html">Using contralateral referencing for EEG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/css.html">Cortical Signal Suppression (CSS) for removal of cortical signals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/define_target_events.html">Define target events based on time lag, plot evoked response</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/eeg_bridging.html">Identify EEG Electrodes Bridged by too much Gel</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/eeg_csd.html">Transform EEG data using current source density (CSD)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/eog_artifact_histogram.html">Show EOG artifact timing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/eog_regression.html">Reduce EOG artifacts through regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/epochs_metadata.html">Automated epochs metadata generation with variable time windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/esg_rm_heart_artefact_pcaobs.html">Principal Component Analysis - Optimal Basis Sets (PCA-OBS) removing cardiac artefact</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/find_ref_artifacts.html">Find MEG reference channel artifacts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/fnirs_artifact_removal.html">Visualise NIRS artifact correction methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/ica_comparison.html">Compare the different ICA algorithms in MNE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/interpolate_bad_channels.html">Interpolate bad channels for MEG/EEG channels</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/interpolate_to.html">Interpolate EEG data to any montage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/movement_compensation.html">Maxwell filter data with movement compensation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/movement_detection.html">Annotate movement artifacts and reestimate dev_head_t</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/muscle_detection.html">Annotate muscle artifacts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/muscle_ica.html">Removing muscle ICA components</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/otp.html">Plot sensor denoising using oversampled temporal projection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/shift_evoked.html">Shifting time-scale in evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/virtual_evoked.html">Remap MEG channel types</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/preprocessing/xdawn_denoising.html">XDAWN Denoising</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/visualization/index.html">Visualization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/3d_to_2d.html">How to convert 3D electrode positions to a 2D image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/brain.html">Plotting with <code class="docutils literal notranslate"><span class="pre">mne.viz.Brain</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/channel_epochs_image.html">Visualize channel over epochs as an image</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/eeg_on_scalp.html">Plotting EEG sensors on the scalp</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/evoked_arrowmap.html">Plotting topographic arrowmaps of evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/evoked_topomap.html">Plotting topographic maps of evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/evoked_whitening.html">Whitening evoked data with a noise covariance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/eyetracking_plot_heatmap.html">Plotting eye-tracking heatmaps in MNE-Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/meg_sensors.html">Plotting sensor layouts of MEG systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/mne_helmet.html">Plot the MNE brain and helmet</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/montage_sgskip.html">Plotting sensor layouts of EEG systems</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/parcellation.html">Plot a cortical parcellation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/roi_erpimage_by_rt.html">Plot single trial activity, grouped by ROI and sorted by RT</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/ssp_projs_sensitivity_map.html">Sensitivity map of SSP projections</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/topo_compare_conditions.html">Compare evoked responses for different conditions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/topo_customized.html">Plot custom topographies for MEG sensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/visualization/xhemi.html">Cross-hemisphere comparison</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/time_frequency/index.html">Time-Frequency Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/compute_csd.html">Compute a cross-spectral density (CSD) matrix</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/compute_source_psd_epochs.html">Compute Power Spectral Density of inverse solution from single epochs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/source_label_time_frequency.html">Compute power and phase lock in label of the source space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/source_power_spectrum.html">Compute source power spectral density (PSD) in a label</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/source_power_spectrum_opm.html">Compute source power spectral density (PSD) of VectorView and OPM data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/source_space_time_frequency.html">Compute induced power in the source space with dSPM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/temporal_whitening.html">Temporal whitening with AR model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/time_frequency_erds.html">Compute and visualize ERDS maps</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/time_frequency_global_field_power.html">Explore event-related dynamics for specific frequency bands</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/time_frequency/time_frequency_simulated.html">Time-frequency on simulated data (Multitaper vs. Morlet vs. Stockwell vs. Hilbert)</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/stats/index.html">Statistics Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/stats/cluster_stats_evoked.html">Permutation F-test on sensor data with 1D cluster level</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/stats/fdr_stats_evoked.html">FDR correction on T-test on sensor data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/stats/linear_regression_raw.html">Regression on continuous data (rER[P/F])</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/stats/sensor_permutation_test.html">Permutation T-test on sensor data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/stats/sensor_regression.html">Analysing continuous features with binning and regression in sensor space</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/decoding/index.html">Machine Learning (Decoding, Encoding, and MVPA)</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_csp_eeg.html">Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_csp_timefreq.html">Decoding in time-frequency space using Common Spatial Patterns (CSP)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_rsa_sgskip.html">Representational Similarity Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_spatio_temporal_source.html">Decoding source space data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_spoc_CMC.html">Continuous Target Decoding with SPoC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_time_generalization_conditions.html">Decoding sensor space data with generalization across time and conditions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_unsupervised_spatial_filter.html">Analysis of evoked response using ICA and PCA reduction techniques</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/decoding_xdawn_eeg.html">XDAWN Decoding From EEG data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/ems_filtering.html">Compute effect-matched-spatial filtering (EMS)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/linear_model_patterns.html">Linear classifier on sensor data with plot patterns and filters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/receptive_field_mtrf.html">Receptive Field Estimation and Prediction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/decoding/ssd_spatial_filters.html">Compute spatial filters with Spatio-Spectral Decomposition (SSD)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../auto_examples/connectivity/index.html">Connectivity Analysis Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/forward/index.html">Forward modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/forward/forward_sensitivity_maps.html">Display sensitivity maps for EEG and MEG sensors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/forward/left_cerebellum_volume_source.html">Generate a left cerebellum volume source space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/forward/source_space_morphing.html">Use source space morphing</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/inverse/index.html">Inverse problem and source analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/compute_mne_inverse_epochs_in_label.html">Compute MNE-dSPM inverse solution on single epochs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/compute_mne_inverse_raw_in_label.html">Compute sLORETA inverse solution on raw data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/compute_mne_inverse_volume.html">Compute MNE-dSPM inverse solution on evoked data in volume source space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/custom_inverse_solver.html">Source localization with a custom inverse solver</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/dics_epochs.html">Compute source level time-frequency timecourses using a DICS beamformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/dics_source_power.html">Compute source power using DICS beamformer</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/evoked_ers_source_power.html">Compute evoked ERS source power using DICS, LCMV beamformer, and dSPM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/gamma_map_inverse.html">Compute a sparse inverse solution using the Gamma-MAP empirical Bayesian method</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/label_activation_from_stc.html">Extracting time course from source_estimate object</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/label_from_stc.html">Generate a functional label from source estimates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/label_source_activations.html">Extracting the time series of activations in a label</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/mixed_norm_inverse.html">Compute sparse inverse solution with mixed norm: MxNE and irMxNE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/mixed_source_space_inverse.html">Compute MNE inverse solution on evoked data with a mixed source space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/mne_cov_power.html">Compute source power estimate by projecting the covariance with MNE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/morph_surface_stc.html">Morph surface source estimate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/morph_volume_stc.html">Morph volumetric source estimate</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/multi_dipole_model.html">Computing source timecourses with an XFit-like multi-dipole model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/multidict_reweighted_tfmxne.html">Compute iterative reweighted TF-MxNE with multiscale time-frequency dictionary</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/psf_ctf_label_leakage.html">Visualize source leakage among labels using a circular graph</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/psf_ctf_vertices.html">Plot point-spread functions (PSFs) and cross-talk functions (CTFs)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/psf_ctf_vertices_lcmv.html">Compute cross-talk functions for LCMV beamformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/psf_volume.html">Plot point-spread functions (PSFs) for a volume</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/rap_music.html">Compute Rap-Music on evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/read_inverse.html">Reading an inverse operator</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/read_stc.html">Reading an STC file</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/resolution_metrics.html">Compute spatial resolution metrics in source space</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/resolution_metrics_eegmeg.html">Compute spatial resolution metrics to compare MEG with EEG+MEG</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/snr_estimate.html">Estimate data SNR using an inverse</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/source_space_snr.html">Computing source space SNR</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/time_frequency_mixed_norm_inverse.html">Compute MxNE with time-frequency sparse prior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/trap_music.html">Compute Trap-Music on evoked data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/inverse/vector_mne_solution.html">Plotting the full vector-valued MNE solution</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../auto_examples/datasets/index.html">Examples on open datasets</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/brainstorm_data.html">Brainstorm raw (median nerve) dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/hf_sef_data.html">HF-SEF dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/kernel_phantom.html">Kernel OPM phantom data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/limo_data.html">Single trial linear regression analysis with the LIMO dataset</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/opm_data.html">Optically pumped magnetometer (OPM) data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../auto_examples/datasets/spm_faces_dataset.html">From raw data to dSPM on SPM Faces dataset</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="implementation.html">Implementation details</a></li>
<li class="toctree-l1"><a class="reference internal" href="design_philosophy.html">Design philosophy</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Example datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../generated/commands.html">Command-line tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../help/migrating.html">Migrating from other analysis software</a></li>
<li class="toctree-l1"><a class="reference internal" href="cookbook.html">The typical M/EEG workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="cite.html">How to cite MNE-Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="cited.html">Papers citing MNE-Python</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none"></div>
              
              
<div>
  
  <section id="datasets-overview">
<span id="datasets"></span><h1>Datasets Overview<a class="headerlink" href="#datasets-overview" title="Link to this heading">#</a></h1>
<div class="sidebar admonition note">
<p class="admonition-title">Note</p>
<p>Contributing datasets to MNE-Python</p>
<p>Do not hesitate to contact MNE-Python developers on the
<a class="reference external" href="https://mne.discourse.group">MNE Forum</a> to discuss the possibility of
adding more publicly available datasets.</p>
</div>
<p>All the dataset fetchers are available in <a class="reference internal" href="../api/datasets.html#module-mne.datasets" title="mne.datasets"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mne.datasets</span></code></a>. To download any of the datasets,
use the <code class="docutils literal notranslate"><span class="pre">data_path</span></code> (fetches full dataset) or the <code class="docutils literal notranslate"><span class="pre">load_data</span></code> (fetches dataset partially) functions.</p>
<p>All fetchers will check the default download location first to see if the dataset
is already on your computer, and only download it if necessary. The default
download location is also configurable; see the documentation of any of the
<code class="docutils literal notranslate"><span class="pre">data_path</span></code> functions for more information.</p>
<section id="sample">
<span id="sample-dataset"></span><h2>Sample<a class="headerlink" href="#sample" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path" title="mne.datasets.sample.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.sample.data_path()</span></code></a></p>
<p>These data were acquired with the Neuromag
Vectorview system at MGH/HMS/MIT Athinoula A. Martinos Center Biomedical
Imaging. EEG data from a 60-channel electrode cap was acquired simultaneously with
the MEG. The original MRI data set was acquired with a Siemens 1.5 T
Sonata scanner using an MPRAGE sequence.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These data are provided solely for the purpose of getting familiar
with the MNE software. The data should not be used to evaluate the
performance of the MEG or MRI system employed.</p>
</div>
<p>In this experiment, checkerboard patterns were presented to the subject
into the left and right visual field, interspersed by tones to the
left or right ear. The interval between the stimuli was 750 ms. Occasionally
a smiley face was presented at the center of the visual field.
The subject was asked to press a key with the right index finger
as soon as possible after the appearance of the face.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id23">
<caption><span class="caption-text">Trigger codes for the sample data set.</span><a class="headerlink" href="#id23" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"></th>
<th class="head"><p>Contents</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LA</p></td>
<td><p>1</p></td>
<td><p>Response to left-ear auditory stimulus</p></td>
</tr>
<tr class="row-odd"><td><p>RA</p></td>
<td><p>2</p></td>
<td><p>Response to right-ear auditory stimulus</p></td>
</tr>
<tr class="row-even"><td><p>LV</p></td>
<td><p>3</p></td>
<td><p>Response to left visual field stimulus</p></td>
</tr>
<tr class="row-odd"><td><p>RV</p></td>
<td><p>4</p></td>
<td><p>Response to right visual field stimulus</p></td>
</tr>
<tr class="row-even"><td><p>smiley</p></td>
<td><p>5</p></td>
<td><p>Response to the smiley face</p></td>
</tr>
<tr class="row-odd"><td><p>button</p></td>
<td><p>32</p></td>
<td><p>Response triggered by the button press</p></td>
</tr>
</tbody>
</table>
</div>
<section id="contents-of-the-data-set">
<h3>Contents of the data set<a class="headerlink" href="#contents-of-the-data-set" title="Link to this heading">#</a></h3>
<p>The sample data set contains two main directories: <code class="docutils literal notranslate"><span class="pre">MEG/sample</span></code> (the MEG/EEG
data) and <code class="docutils literal notranslate"><span class="pre">subjects/sample</span></code> (the MRI reconstructions).
In addition to subject <code class="docutils literal notranslate"><span class="pre">sample</span></code>, the MRI surface reconstructions from another
subject, morph, are provided to demonstrate morphing capabilities.</p>
<div class="pst-scrollable-table-container"><table class="table" id="id24">
<caption><span class="caption-text">Contents of the MEG/sample directory.</span><a class="headerlink" href="#id24" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>File</p></th>
<th class="head"><p>Contents</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>sample/audvis_raw.fif</p></td>
<td><p>The raw MEG/EEG data</p></td>
</tr>
<tr class="row-odd"><td><p>audvis.ave</p></td>
<td><p>A template script for off-line averaging</p></td>
</tr>
<tr class="row-even"><td><p>auvis.cov</p></td>
<td><p>A template script for the computation of a noise-covariance matrix</p></td>
</tr>
</tbody>
</table>
</div>
<div class="pst-scrollable-table-container"><table class="table" id="id25">
<caption><span class="caption-text">Overview of the contents of the subjects/sample directory.</span><a class="headerlink" href="#id25" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>File / directory</p></th>
<th class="head"><p>Contents</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>bem</p></td>
<td><p>Directory for the forward modelling data</p></td>
</tr>
<tr class="row-odd"><td><p>bem/watershed</p></td>
<td><p>BEM surface segmentation data computed with the watershed algorithm</p></td>
</tr>
<tr class="row-even"><td><p>bem/inner_skull.surf</p></td>
<td><p>Inner skull surface for BEM</p></td>
</tr>
<tr class="row-odd"><td><p>bem/outer_skull.surf</p></td>
<td><p>Outer skull surface for BEM</p></td>
</tr>
<tr class="row-even"><td><p>bem/outer_skin.surf</p></td>
<td><p>Skin surface for BEM</p></td>
</tr>
<tr class="row-odd"><td><p>sample-head.fif</p></td>
<td><p>Skin surface in fif format for mne_analyze visualizations</p></td>
</tr>
<tr class="row-even"><td><p>surf</p></td>
<td><p>Surface reconstructions</p></td>
</tr>
<tr class="row-odd"><td><p>mri/T1</p></td>
<td><p>The T1-weighted MRI data employed in visualizations</p></td>
</tr>
</tbody>
</table>
</div>
<p>The following preprocessing steps have been already accomplished
in the sample data set:</p>
<ul class="simple">
<li><p>The MRI surface reconstructions have
been computed using the FreeSurfer software.</p></li>
<li><p>The BEM surfaces have been created with the watershed algorithm,
see <a class="reference internal" href="implementation.html#bem-watershed-algorithm"><span class="std std-ref">Using the watershed algorithm</span></a>.</p></li>
</ul>
<p>The <strong>sample</strong> dataset is distributed with <a class="reference internal" href="#fsaverage"><span class="std std-ref">fsaverage</span></a> for convenience.</p>
</section>
</section>
<section id="ucl-opm-auditory">
<span id="ucl-opm-auditory-dataset"></span><h2>UCL OPM Auditory<a class="headerlink" href="#ucl-opm-auditory" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.ucl_opm_auditory.data_path.html#mne.datasets.ucl_opm_auditory.data_path" title="mne.datasets.ucl_opm_auditory.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.ucl_opm_auditory.data_path()</span></code></a>.</p>
<p>A basic auditory evoked field experiment using an OPM setup from FIL at UCL.
See <a class="footnote-reference brackets" href="#footcite-seymouretal2022" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> for details.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/preprocessing/80_opm_processing.html#tut-opm-processing"><span class="std std-ref">Preprocessing optically pumped magnetometer (OPM) MEG data</span></a></p></li>
</ul>
</aside>
</section>
<section id="brainstorm">
<h2>Brainstorm<a class="headerlink" href="#brainstorm" title="Link to this heading">#</a></h2>
<p>Dataset fetchers for three Brainstorm tutorials are available. Users must agree to the
license terms of these datasets before downloading them. These files are recorded in a CTF 275 system
and are provided in native CTF format (.ds files).</p>
<section id="auditory">
<h3>Auditory<a class="headerlink" href="#auditory" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.brainstorm.bst_raw.data_path.html#mne.datasets.brainstorm.bst_raw.data_path" title="mne.datasets.brainstorm.bst_raw.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.brainstorm.bst_raw.data_path()</span></code></a>.</p>
<p>Details about the data can be found at the Brainstorm <a class="reference external" href="https://neuroimage.usc.edu/brainstorm/DatasetAuditory">auditory dataset tutorial</a>.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/io/60_ctf_bst_auditory.html#tut-brainstorm-auditory"><span class="std std-ref">Working with CTF data: the Brainstorm auditory dataset</span></a>: Partially replicates the original Brainstorm tutorial.</p></li>
</ul>
</aside>
</section>
<section id="resting-state">
<h3>Resting state<a class="headerlink" href="#resting-state" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.brainstorm.bst_resting.data_path.html#mne.datasets.brainstorm.bst_resting.data_path" title="mne.datasets.brainstorm.bst_resting.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.brainstorm.bst_resting.data_path()</span></code></a></p>
<p>Details can be found at the Brainstorm <a class="reference external" href="https://neuroimage.usc.edu/brainstorm/DatasetResting">resting state dataset tutorial</a>.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference external" href="https://mne.tools/mne-connectivity/stable/auto_examples/mne_inverse_envelope_correlation.html#ex-envelope-correlation" title="(in MNE-Connectivity v0.7)"><span>Compute envelope correlations in source space</span></a></p></li>
</ul>
</aside>
</section>
<section id="median-nerve">
<h3>Median nerve<a class="headerlink" href="#median-nerve" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.brainstorm.bst_raw.data_path.html#mne.datasets.brainstorm.bst_raw.data_path" title="mne.datasets.brainstorm.bst_raw.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.brainstorm.bst_raw.data_path()</span></code></a></p>
<p>Details can be found at the Brainstorm <a class="reference external" href="https://neuroimage.usc.edu/brainstorm/DatasetMedianNerveCtf">median nerve dataset tutorial</a>.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/datasets/brainstorm_data.html#ex-brainstorm-raw"><span class="std std-ref">Brainstorm raw (median nerve) dataset</span></a></p></li>
</ul>
</aside>
</section>
</section>
<section id="spm-faces">
<h2>SPM faces<a class="headerlink" href="#spm-faces" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.spm_face.data_path.html#mne.datasets.spm_face.data_path" title="mne.datasets.spm_face.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.spm_face.data_path()</span></code></a></p>
<p>The <a class="reference external" href="https://www.fil.ion.ucl.ac.uk/spm/data/mmfaces/">SPM faces dataset</a> contains EEG, MEG and fMRI recordings on face perception.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/datasets/spm_faces_dataset.html#ex-spm-faces"><span class="std std-ref">From raw data to dSPM on SPM Faces dataset</span></a> Full pipeline including artifact removal, epochs averaging, forward model computation and source reconstruction using dSPM on the contrast: “faces - scrambled”.</p></li>
</ul>
</aside>
</section>
<section id="eegbci-motor-imagery">
<h2>EEGBCI motor imagery<a class="headerlink" href="#eegbci-motor-imagery" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.eegbci.load_data.html#mne.datasets.eegbci.load_data" title="mne.datasets.eegbci.load_data"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.eegbci.load_data()</span></code></a></p>
<p>The EEGBCI dataset is documented in <a class="footnote-reference brackets" href="#footcite-schalketal2004" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> and on the
<a class="reference external" href="https://physionet.org/content/eegmmidb/1.0.0/">PhysioNet documentation page</a>.
The data set is available at PhysioNet <a class="footnote-reference brackets" href="#footcite-goldbergeretal2000" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.
It contains 64-channel EEG recordings from 109 subjects and 14 runs on each
subject in EDF+ format. The recordings were made using the BCI2000 system.
To load a subject, do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">mne.io</span><span class="w"> </span><span class="kn">import</span> <span class="n">concatenate_raws</span><span class="p">,</span> <span class="n">read_raw_edf</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mne.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">eegbci</span>
<span class="n">subjects</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># may vary</span>
<span class="n">runs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>  <span class="c1"># may vary</span>
<span class="n">raw_fnames</span> <span class="o">=</span> <span class="n">eegbci</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">subjects</span><span class="p">,</span> <span class="n">runs</span><span class="p">)</span>
<span class="n">raws</span> <span class="o">=</span> <span class="p">[</span><span class="n">read_raw_edf</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">preload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">raw_fnames</span><span class="p">]</span>
<span class="c1"># concatenate runs from subject</span>
<span class="n">raw</span> <span class="o">=</span> <span class="n">concatenate_raws</span><span class="p">(</span><span class="n">raws</span><span class="p">)</span>
<span class="c1"># make channel names follow standard conventions</span>
<span class="n">eegbci</span><span class="o">.</span><span class="n">standardize</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>
</pre></div>
</div>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decoding/decoding_csp_eeg.html#ex-decoding-csp-eeg"><span class="std std-ref">Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)</span></a></p></li>
</ul>
</aside>
</section>
<section id="somatosensory">
<span id="somato-dataset"></span><h2>Somatosensory<a class="headerlink" href="#somatosensory" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.somato.data_path.html#mne.datasets.somato.data_path" title="mne.datasets.somato.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.somato.data_path()</span></code></a></p>
<p>This dataset contains somatosensory data with event-related synchronizations
(ERS) and desynchronizations (ERD).</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/time-freq/20_sensors_time_frequency.html#tut-sensors-time-freq"><span class="std std-ref">Frequency and time-frequency sensor analysis</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/inverse/dics_source_power.html#ex-inverse-source-power"><span class="std std-ref">Compute source power using DICS beamformer</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/time_frequency/time_frequency_global_field_power.html#ex-time-freq-global-field-power"><span class="std std-ref">Explore event-related dynamics for specific frequency bands</span></a></p></li>
</ul>
</aside>
</section>
<section id="multimodal">
<h2>Multimodal<a class="headerlink" href="#multimodal" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.multimodal.data_path.html#mne.datasets.multimodal.data_path" title="mne.datasets.multimodal.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.multimodal.data_path()</span></code></a></p>
<p>This dataset contains a single subject recorded at Otaniemi (Aalto University)
with auditory, visual, and somatosensory stimuli.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/io/elekta_epochs.html#ex-io-ave-fiff"><span class="std std-ref">Getting averaging info from .fif files</span></a></p></li>
</ul>
</aside>
</section>
<section id="fnirs-motor">
<span id="fnirs-motor-dataset"></span><h2>fNIRS motor<a class="headerlink" href="#fnirs-motor" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.fnirs_motor.data_path.html#mne.datasets.fnirs_motor.data_path" title="mne.datasets.fnirs_motor.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.fnirs_motor.data_path()</span></code></a></p>
<p>This dataset contains a single subject recorded at Macquarie University.
It has optodes placed over the motor cortex. There are three conditions:</p>
<ul class="simple">
<li><p>tapping the left thumb to fingers</p></li>
<li><p>tapping the right thumb to fingers</p></li>
<li><p>a control where nothing happens</p></li>
</ul>
<p>The tapping lasts 5 seconds, and there are 30 trials of each condition.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/preprocessing/70_fnirs_processing.html#tut-fnirs-processing"><span class="std std-ref">Preprocessing functional near-infrared spectroscopy (fNIRS) data</span></a></p></li>
</ul>
</aside>
</section>
<section id="high-frequency-sef">
<h2>High frequency SEF<a class="headerlink" href="#high-frequency-sef" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.hf_sef.data_path.html#mne.datasets.hf_sef.data_path" title="mne.datasets.hf_sef.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.hf_sef.data_path()</span></code></a></p>
<p>This dataset contains somatosensory evoked fields (median nerve stimulation)
with thousands of epochs. It was recorded with an Elekta TRIUX MEG device at
a sampling frequency of 3 kHz. The dataset is suitable for investigating
high-frequency somatosensory responses. Data from two subjects are included
with MRI images in DICOM format and FreeSurfer reconstructions.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/datasets/hf_sef_data.html#ex-hf-sef-data"><span class="std std-ref">high-frequency SEF responses</span></a>.</p></li>
</ul>
</aside>
</section>
<section id="visual-92-object-categories">
<h2>Visual 92 object categories<a class="headerlink" href="#visual-92-object-categories" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.visual_92_categories.data_path.html#mne.datasets.visual_92_categories.data_path" title="mne.datasets.visual_92_categories.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.visual_92_categories.data_path()</span></code></a>.</p>
<p>This dataset is recorded using a 306-channel Neuromag vectorview system.</p>
<p>Experiment consisted in the visual presentation of 92 images of human, animal
and inanimate objects either natural or artificial <a class="footnote-reference brackets" href="#footcite-cichyetal2014" id="id7" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.
Given the high number of conditions this dataset is well adapted to an approach
based on Representational Similarity Analysis (RSA).</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decoding/decoding_rsa_sgskip.html#ex-rsa-noplot"><span class="std std-ref">Representational Similarity Analysis (RSA)</span></a>: Partially replicates the results from <a class="footnote-reference brackets" href="#footcite-cichyetal2014" id="id8" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a>.</p></li>
</ul>
</aside>
</section>
<section id="mtrf-dataset">
<h2>mTRF Dataset<a class="headerlink" href="#mtrf-dataset" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.mtrf.data_path.html#mne.datasets.mtrf.data_path" title="mne.datasets.mtrf.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.mtrf.data_path()</span></code></a>.</p>
<p>This dataset contains 128 channel EEG as well as natural speech stimulus features,
which is also available <a class="reference external" href="https://sourceforge.net/projects/aespa/files/">here</a>.</p>
<p>The experiment consisted of subjects listening to natural speech.
The dataset contains several feature representations of the speech stimulus,
suitable for using to fit continuous regression models of neural activity.
More details and a description of the package can be found in
<a class="footnote-reference brackets" href="#footcite-crosseetal2016" id="id9" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decoding/receptive_field_mtrf.html#ex-receptive-field-mtrf"><span class="std std-ref">Receptive Field Estimation and Prediction</span></a>: Partially replicates the results from <a class="footnote-reference brackets" href="#footcite-crosseetal2016" id="id10" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a>.</p></li>
</ul>
</aside>
</section>
<section id="kiloword-dataset">
<span id="id11"></span><h2>Kiloword dataset<a class="headerlink" href="#kiloword-dataset" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.kiloword.data_path.html#mne.datasets.kiloword.data_path" title="mne.datasets.kiloword.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.kiloword.data_path()</span></code></a>.</p>
<p>This dataset consists of averaged EEG data from 75 subjects performing a
lexical decision task on 960 English words <a class="footnote-reference brackets" href="#footcite-dufauetal2015" id="id12" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a>. The words
are richly annotated, and can be used for e.g. multiple regression estimation
of EEG correlates of printed word processing.</p>
</section>
<section id="kit-phantom-dataset">
<h2>KIT phantom dataset<a class="headerlink" href="#kit-phantom-dataset" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.phantom_kit.data_path.html#mne.datasets.phantom_kit.data_path" title="mne.datasets.phantom_kit.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.phantom_kit.data_path()</span></code></a>.</p>
<p>This dataset was obtained with a phantom on a KIT system at
Macquarie University in Sydney, Australia.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/inverse/95_phantom_KIT.html#tut-phantom-kit"><span class="std std-ref">KIT phantom dataset tutorial</span></a></p></li>
</ul>
</aside>
</section>
<section id="d-neuroimaging-bti-dataset">
<h2>4D Neuroimaging / BTi dataset<a class="headerlink" href="#d-neuroimaging-bti-dataset" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.phantom_4dbti.data_path.html#mne.datasets.phantom_4dbti.data_path" title="mne.datasets.phantom_4dbti.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.phantom_4dbti.data_path()</span></code></a>.</p>
<p>This dataset was obtained with a phantom on a 4D Neuroimaging / BTi system at
the MEG center in La Timone hospital in Marseille.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/inverse/90_phantom_4DBTi.html#tut-phantom-4dbti"><span class="std std-ref">4D Neuroimaging/BTi phantom dataset tutorial</span></a></p></li>
</ul>
</aside>
</section>
<section id="kernel-opm-phantom-dataset">
<h2>Kernel OPM phantom dataset<a class="headerlink" href="#kernel-opm-phantom-dataset" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.phantom_kernel.data_path.html#mne.datasets.phantom_kernel.data_path" title="mne.datasets.phantom_kernel.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.phantom_kernel.data_path()</span></code></a>.</p>
<p>This dataset was obtained with a Neuromag phantom in a Kernel Flux (720-sensor)
system at ILABS at the University of Washington. Only 7 out of 42 possible modules
were active for testing purposes, yielding 121 channels of data with limited coverage
(mostly occipital and parietal).</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/datasets/kernel_phantom.html#ex-kernel-opm-phantom"><span class="std std-ref">Kernel OPM phantom data</span></a></p></li>
</ul>
</aside>
</section>
<section id="opm">
<h2>OPM<a class="headerlink" href="#opm" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.opm.data_path.html#mne.datasets.opm.data_path" title="mne.datasets.opm.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.opm.data_path()</span></code></a></p>
<p>OPM data acquired using an Elekta DACQ, simply piping the data into Elekta
magnetometer channels. The FIF files thus appear to come from a TRIUX system
that is only acquiring a small number of magnetometer channels instead of the
whole array.</p>
<p>The OPM <code class="docutils literal notranslate"><span class="pre">coil_type</span></code> is custom, requiring a custom <code class="docutils literal notranslate"><span class="pre">coil_def.dat</span></code>.
The new <code class="docutils literal notranslate"><span class="pre">coil_type</span></code> is 9999.</p>
<p>OPM co-registration differs a bit from the typical SQUID-MEG workflow.
No <code class="docutils literal notranslate"><span class="pre">-trans.fif</span></code> file is needed for the OPMs, the FIF files include proper
sensor locations in MRI coordinates and no digitization of RPA/LPA/Nasion.
Thus the MEG&lt;-&gt;Head coordinate transform is taken to be an identity matrix
(i.e., everything is in MRI coordinates), even though this mis-identifies
the head coordinate frame (which is defined by the relationship of the
LPA, RPA, and Nasion).</p>
<p>Triggers include:</p>
<ul class="simple">
<li><p>Median nerve stimulation: trigger value 257.</p></li>
<li><p>Magnetic trigger (in OPM measurement only): trigger value 260.
1 second before the median nerve stimulation, a magnetic trigger is piped into the MSR.
This was to be able to check the synchronization between OPMs retrospectively, as each
sensor runs on an independent clock. Synchronization turned out to be satisfactory.</p></li>
</ul>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/datasets/opm_data.html#ex-opm-somatosensory"><span class="std std-ref">Optically pumped magnetometer (OPM) data</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/time_frequency/source_power_spectrum_opm.html#ex-opm-resting-state"><span class="std std-ref">Compute source power spectral density (PSD) of VectorView and OPM data</span></a></p></li>
</ul>
</aside>
</section>
<section id="the-sleep-polysomnographic-database">
<h2>The Sleep PolySomnoGraphic Database<a class="headerlink" href="#the-sleep-polysomnographic-database" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.sleep_physionet.age.fetch_data.html#mne.datasets.sleep_physionet.age.fetch_data" title="mne.datasets.sleep_physionet.age.fetch_data"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.sleep_physionet.age.fetch_data()</span></code></a>
<a class="reference internal" href="../generated/mne.datasets.sleep_physionet.temazepam.fetch_data.html#mne.datasets.sleep_physionet.temazepam.fetch_data" title="mne.datasets.sleep_physionet.temazepam.fetch_data"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.sleep_physionet.temazepam.fetch_data()</span></code></a></p>
<p>The sleep PhysioNet database contains 197 whole-night PolySomnoGraphic sleep
recordings, containing EEG, EOG, chin EMG, and event markers. Some records also
contain respiration and body temperature. Corresponding hypnograms (sleep
patterns) were manually scored by well-trained technicians according to the
Rechtschaffen and Kales manual, and are also available. If you use these
data please cite <a class="footnote-reference brackets" href="#footcite-kempetal2000" id="id13" role="doc-noteref"><span class="fn-bracket">[</span>7<span class="fn-bracket">]</span></a> and <a class="footnote-reference brackets" href="#footcite-goldbergeretal2000" id="id14" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a>.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/clinical/60_sleep.html#tut-sleep-stage-classif"><span class="std std-ref">Sleep stage classification from polysomnography (PSG) data</span></a></p></li>
</ul>
</aside>
</section>
<section id="reference-channel-noise-meg-data-set">
<h2>Reference channel noise MEG data set<a class="headerlink" href="#reference-channel-noise-meg-data-set" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.refmeg_noise.data_path.html#mne.datasets.refmeg_noise.data_path" title="mne.datasets.refmeg_noise.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.refmeg_noise.data_path()</span></code></a>.</p>
<p>This dataset was obtained with a 4D Neuroimaging / BTi system at
the University Clinic - Erlangen, Germany. There are powerful bursts of
external magnetic noise throughout the recording, which make it a good
example for automatic noise removal techniques.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/preprocessing/find_ref_artifacts.html#ex-megnoise-processing"><span class="std std-ref">Find MEG reference channel artifacts</span></a></p></li>
</ul>
</aside>
</section>
<section id="miscellaneous-datasets">
<h2>Miscellaneous Datasets<a class="headerlink" href="#miscellaneous-datasets" title="Link to this heading">#</a></h2>
<p>These datasets are used for specific purposes in the documentation and in
general are not useful for separate analyses.</p>
<section id="fsaverage">
<span id="id15"></span><h3>fsaverage<a class="headerlink" href="#fsaverage" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.fetch_fsaverage.html#mne.datasets.fetch_fsaverage" title="mne.datasets.fetch_fsaverage"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.fetch_fsaverage()</span></code></a></p>
<p>For convenience, we provide a function to separately download and extract the
(or update an existing) fsaverage subject. See also the
<a class="reference internal" href="../auto_tutorials/forward/10_background_freesurfer.html#fsaverage-background"><span class="std std-ref">background information on fsaverage</span></a>.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<p><a class="reference internal" href="../auto_tutorials/forward/35_eeg_no_mri.html#tut-eeg-fsaverage-source-modeling"><span class="std std-ref">EEG forward operator with a template MRI</span></a></p>
</aside>
</section>
<section id="infant-template-mris">
<h3>Infant template MRIs<a class="headerlink" href="#infant-template-mris" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.fetch_infant_template.html#mne.datasets.fetch_infant_template" title="mne.datasets.fetch_infant_template"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.fetch_infant_template()</span></code></a></p>
<p>This function will download an infant template MRI from
<a class="footnote-reference brackets" href="#footcite-oreillyetal2021" id="id16" role="doc-noteref"><span class="fn-bracket">[</span>8<span class="fn-bracket">]</span></a> along with MNE-specific files.</p>
</section>
<section id="ecog-dataset">
<h3>ECoG Dataset<a class="headerlink" href="#ecog-dataset" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.misc.data_path.html#mne.datasets.misc.data_path" title="mne.datasets.misc.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.misc.data_path()</span></code></a>. Data exists at <code class="docutils literal notranslate"><span class="pre">/ecog/</span></code>.</p>
<p>This dataset contains a sample electrocorticography (ECoG) dataset. It includes
two grids of electrodes and ten shaft electrodes with simulated motor data (actual data
pending availability).</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/visualization/3d_to_2d.html#ex-electrode-pos-2d"><span class="std std-ref">How to convert 3D electrode positions to a 2D image</span></a>: Demonstrates how to project a 3D electrode location onto a 2D image, a common procedure in ECoG analyses.</p></li>
<li><p><a class="reference external" href="https://mne.tools/mne-gui-addons/auto_examples/ieeg_locate.html#tut-ieeg-localize" title="(in MNE-GUI-Addons v0.2)"><span>Locating intracranial electrode contacts</span></a>: Demonstrates how to use a graphical user interface to locate electrode contacts as well as warp them to a common atlas.</p></li>
</ul>
</aside>
</section>
<section id="seeg-dataset">
<h3>sEEG Dataset<a class="headerlink" href="#seeg-dataset" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.misc.data_path.html#mne.datasets.misc.data_path" title="mne.datasets.misc.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.misc.data_path()</span></code></a>. Data exists at <code class="docutils literal notranslate"><span class="pre">/seeg/</span></code>.</p>
<p>This dataset contains a sample stereoelectroencephalography (sEEG) dataset.
It includes 21 shaft electrodes during a two-choice movement task on a keyboard.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference external" href="https://mne.tools/mne-gui-addons/auto_examples/ieeg_locate.html#tut-ieeg-localize" title="(in MNE-GUI-Addons v0.2)"><span>Locating intracranial electrode contacts</span></a>: Demonstrates how to use a graphical user interface to locate electrode contacts as well as warp them to a common atlas.</p></li>
<li><p><a class="reference internal" href="../auto_tutorials/clinical/20_seeg.html#tut-working-with-seeg"><span class="std std-ref">Working with sEEG data</span></a>: Demonstrates ways to plot sEEG anatomy and results.</p></li>
</ul>
</aside>
</section>
<section id="limo-dataset">
<span id="id17"></span><h3>LIMO Dataset<a class="headerlink" href="#limo-dataset" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.limo.load_data.html#mne.datasets.limo.load_data" title="mne.datasets.limo.load_data"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.limo.load_data()</span></code></a>.</p>
<p>In the original LIMO experiment (see <a class="footnote-reference brackets" href="#footcite-rousseletetal2010" id="id18" role="doc-noteref"><span class="fn-bracket">[</span>9<span class="fn-bracket">]</span></a>), participants
performed a
two-alternative forced choice task, discriminating between two face stimuli.
Subjects discriminated the same two faces during the whole experiment.
The critical manipulation consisted of the level of noise added to the
face-stimuli during the task, making the faces more or less discernible to the
observer.</p>
<p>The presented faces varied across a noise-signal (or phase-coherence) continuum
spanning from 0 to 100% in increasing steps of 10%. In other words, faces with
high phase-coherence (e.g., 90%) were easy to identify, while faces with low
phase-coherence (e.g., 10%) were hard to identify and by extension hard to
discriminate.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/datasets/limo_data.html#ex-limo-data"><span class="std std-ref">Single trial linear regression analysis with the LIMO dataset</span></a>: Explores data from a single subject of the LIMO dataset
and demonstrates how to fit a single trial linear regression using the
information contained in the metadata of the individual datasets.</p></li>
</ul>
</aside>
</section>
<section id="erp-core-dataset">
<span id="id19"></span><h3>ERP CORE Dataset<a class="headerlink" href="#erp-core-dataset" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.erp_core.data_path.html#mne.datasets.erp_core.data_path" title="mne.datasets.erp_core.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.erp_core.data_path()</span></code></a></p>
<p>The original <a class="reference internal" href="#id19">ERP CORE dataset</a> <a class="footnote-reference brackets" href="#footcite-kappenman2021" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>10<span class="fn-bracket">]</span></a> contains data from
40 participants who completed 6 EEG experiments, carefully crafted to evoke
7 well-known event-related potential (ERP) components.</p>
<p>Currently, the MNE-Python ERP CORE dataset only provides data from one
participant (subject <code class="docutils literal notranslate"><span class="pre">001</span></code>) of the Flankers paradigm, which elicits the
lateralized readiness potential (LRP) and error-related negativity (ERN). The
data provided is <strong>not</strong> the original data from the ERP CORE dataset, but
rather a slightly modified version, designed to demonstrate the Epochs metadata
functionality. For example, we already set the references and montage
correctly, and stored events as Annotations. Data is provided in <code class="docutils literal notranslate"><span class="pre">FIFF</span></code>
format.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/epochs/40_autogenerate_metadata.html#tut-autogenerate-metadata"><span class="std std-ref">Auto-generating Epochs metadata</span></a>: Learn how to auto-generate
<a class="reference internal" href="../generated/mne.Epochs.html#mne.Epochs" title="mne.Epochs"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Epochs</span></code></a> metadata, and visualize the error-related negativity (ERN)
ERP component.</p></li>
</ul>
</aside>
</section>
</section>
<section id="ssvep">
<span id="ssvep-dataset"></span><h2>SSVEP<a class="headerlink" href="#ssvep" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.ssvep.data_path.html#mne.datasets.ssvep.data_path" title="mne.datasets.ssvep.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.ssvep.data_path()</span></code></a></p>
<p>This is a simple example dataset with frequency tagged visual stimulation:
N=2 participants observed checkerboards patterns inverting with a constant
frequency of either 12.0 Hz of 15.0 Hz. 10 trials of 20.0 s length each.
32 channels wet EEG was recorded.</p>
<p>Data format: BrainVision .eeg/.vhdr/.vmrk files organized according to BIDS
standard.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/time-freq/50_ssvep.html#tut-ssvep"><span class="std std-ref">Frequency-tagging: Basic analysis of an SSVEP/vSSR dataset</span></a></p></li>
</ul>
</aside>
</section>
<section id="eyelink">
<span id="eyelink-dataset"></span><h2>EYELINK<a class="headerlink" href="#eyelink" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.eyelink.data_path.html#mne.datasets.eyelink.data_path" title="mne.datasets.eyelink.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.eyelink.data_path()</span></code></a></p>
<p>Two small example datasets of eye-tracking data from SR Research EyeLink.</p>
<section id="eeg-eyetracking">
<h3>EEG-Eyetracking<a class="headerlink" href="#eeg-eyetracking" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.eyelink.data_path.html#mne.datasets.eyelink.data_path" title="mne.datasets.eyelink.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.eyelink.data_path()</span></code></a>. Data exists at <code class="docutils literal notranslate"><span class="pre">/eeg-et/</span></code>.</p>
<p>Contains both EEG (EGI) and eye-tracking (ASCII format) data recorded from a
pupillary light reflex experiment, stored in separate files. 1 participant fixated
on the screen while short light flashes appeared. Event onsets were recorded by a
photodiode attached to the screen and were sent to both the EEG and eye-tracking
systems.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_tutorials/preprocessing/90_eyetracking_data.html#tut-eyetrack"><span class="std std-ref">Working with eye tracker data in MNE-Python</span></a></p></li>
</ul>
</aside>
</section>
<section id="freeviewing">
<h3>Freeviewing<a class="headerlink" href="#freeviewing" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.eyelink.data_path.html#mne.datasets.eyelink.data_path" title="mne.datasets.eyelink.data_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">mne.datasets.eyelink.data_path()</span></code></a>. Data exists at <code class="docutils literal notranslate"><span class="pre">/freeviewing/</span></code>.</p>
<p>Contains eye-tracking data (ASCII format) from 1 participant who was free-viewing a
video of a natural scene. In some videos, the natural scene was pixelated such that
the people in the scene were unrecognizable.</p>
<aside class="topic">
<p class="topic-title">Examples</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/visualization/eyetracking_plot_heatmap.html#tut-eyetrack-heatmap"><span class="std std-ref">Plotting eye-tracking heatmaps in MNE-Python</span></a></p></li>
</ul>
</aside>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id21">
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="footcite-seymouretal2022" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">1</a><span class="fn-bracket">]</span></span>
<p>Robert A. Seymour, Nicholas Alexander, Stephanie Mellor, George C. O’Neill, Tim M. Tierney, Gareth R. Barnes, and Eleanor A. Maguire. Interference suppression techniques for OPM-based MEG: Opportunities and challenges. <em>NeuroImage</em>, 247:118834, February 2022. <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2021.118834">doi:10.1016/j.neuroimage.2021.118834</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-schalketal2004" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">2</a><span class="fn-bracket">]</span></span>
<p>Gerwin Schalk, Dennis J. McFarland, Thilo Hinterberger, Niels Birbaumer, and Jonathan R. Wolpaw. BCI2000: a general-purpose brain-computer interface (BCI) system. <em>IEEE Transactions on Biomedical Engineering</em>, 51(6):1034–1043, 2004. <a class="reference external" href="https://doi.org/10.1109/TBME.2004.827072">doi:10.1109/TBME.2004.827072</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-goldbergeretal2000" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id6">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Ary L. Goldberger, Luis A. N. Amaral, Leon Glass, Jeffrey M. Hausdorff, Plamen Ch. Ivanov, Roger G. Mark, Joseph E. Mietus, George B. Moody, Chung-Kang Peng, and H. Eugene Stanley. PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. <em>Circulation</em>, 2000. <a class="reference external" href="https://doi.org/10.1161/01.CIR.101.23.e215">doi:10.1161/01.CIR.101.23.e215</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-cichyetal2014" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id8">2</a>)</span>
<p>Radoslaw Martin Cichy, Dimitrios Pantazis, and Aude Oliva. Resolving human object recognition in space and time. <em>Nature Neuroscience</em>, 17(3):455–462, 2014. <a class="reference external" href="https://doi.org/10.1038/nn.3635">doi:10.1038/nn.3635</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-crosseetal2016" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Michael J. Crosse, Giovanni M. Di Liberto, Adam Bednar, and Edmund C. Lalor. The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli. <em>Frontiers in Human Neuroscience</em>, 2016. <a class="reference external" href="https://doi.org/10.3389/fnhum.2016.00604">doi:10.3389/fnhum.2016.00604</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-dufauetal2015" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">6</a><span class="fn-bracket">]</span></span>
<p>Stéphane Dufau, Jonathan Grainger, Katherine J. Midgley, and Phillip J. Holcomb. A thousand words are worth a picture: snapshots of printed-word processing in an event-related potential megastudy. <em>Psychological Science</em>, 26(12):1887–1897, 2015. <a class="reference external" href="https://doi.org/10.1177/0956797615603934">doi:10.1177/0956797615603934</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-kempetal2000" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">7</a><span class="fn-bracket">]</span></span>
<p>B. Kemp, A. H. Zwinderman, B. Tuk, H. A. C. Kamphuisen, and J. J. L. Oberyé. Analysis of a sleep-dependent neuronal feedback loop: the slow-wave microcontinuity of the EEG. <em>IEEE Transactions on Biomedical Engineering</em>, 47(9):1185–1194, 2000. <a class="reference external" href="https://doi.org/10.1109/10.867928">doi:10.1109/10.867928</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-oreillyetal2021" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">8</a><span class="fn-bracket">]</span></span>
<p>Christian O’Reilly, Eric Larson, John E. Richards, and Mayada Elsabbagh. Structural templates for imaging EEG cortical sources in infants. <em>NeuroImage</em>, 227:117682, 2021. <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2020.117682">doi:10.1016/j.neuroimage.2020.117682</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-rousseletetal2010" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">9</a><span class="fn-bracket">]</span></span>
<p>Guillaume A. Rousselet, Carl M. Gaspar, Cyril R. Pernet, Jesse S. Husk, Patrick J. Bennett, and Allison B. Sekuler. Healthy aging delays scalp EEG sensitivity to noise in a face discrimination task. <em>Frontiers in Psychology</em>, 1(19):1–14, 2010. <a class="reference external" href="https://doi.org/10.3389/fpsyg.2010.00019">doi:10.3389/fpsyg.2010.00019</a>.</p>
</aside>
<aside class="footnote brackets" id="footcite-kappenman2021" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">10</a><span class="fn-bracket">]</span></span>
<p>Emily S. Kappenman, Jaclyn L. Farrens, Wendy Zhang, Andrew X. Stewart, and Steven J. Luck. ERP CORE: an open resource for human event-related potential research. <em>NeuroImage</em>, 225:117465, 2021. <a class="reference external" href="https://doi.org/10.1016/j.neuroimage.2020.117465">doi:10.1016/j.neuroimage.2020.117465</a>.</p>
</aside>
</aside>
</div>
</section>
</section>


  
</div>

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="design_philosophy.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Design philosophy</p>
      </div>
    </a>
    <a class="right-next"
       href="../generated/commands.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Command line tools using Python</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sample">Sample</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contents-of-the-data-set">Contents of the data set</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ucl-opm-auditory">UCL OPM Auditory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#brainstorm">Brainstorm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auditory">Auditory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#resting-state">Resting state</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#median-nerve">Median nerve</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#spm-faces">SPM faces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eegbci-motor-imagery">EEGBCI motor imagery</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#somatosensory">Somatosensory</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multimodal">Multimodal</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fnirs-motor">fNIRS motor</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#high-frequency-sef">High frequency SEF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visual-92-object-categories">Visual 92 object categories</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mtrf-dataset">mTRF Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kiloword-dataset">Kiloword dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kit-phantom-dataset">KIT phantom dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-neuroimaging-bti-dataset">4D Neuroimaging / BTi dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-opm-phantom-dataset">Kernel OPM phantom dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#opm">OPM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sleep-polysomnographic-database">The Sleep PolySomnoGraphic Database</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reference-channel-noise-meg-data-set">Reference channel noise MEG data set</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#miscellaneous-datasets">Miscellaneous Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fsaverage">fsaverage</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#infant-template-mris">Infant template MRIs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ecog-dataset">ECoG Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#seeg-dataset">sEEG Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limo-dataset">LIMO Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#erp-core-dataset">ERP CORE Dataset</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ssvep">SSVEP</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#eyelink">EYELINK</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eeg-eyetracking">EEG-Eyetracking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#freeviewing">Freeviewing</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

    <script src="https://mne.tools/versionwarning.js"></script>
  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item"><p class="text-center small">&copy; Copyright 2012–2025, MNE Developers. Last updated <time datetime="2025-08-26T15:31:16.899671+00:00" class="localized">2025-08-26 15:31 UTC</time>
<script type="text/javascript">function formatTimestamp() {document.querySelectorAll("time.localized").forEach(el => el.textContent = new Date(el.getAttribute("datetime")).toLocaleString([], {dateStyle: "medium", timeStyle: "long"}));};if (document.readyState != "loading") formatTimestamp();else document.addEventListener("DOMContentLoaded", formatTimestamp);</script></p></div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>