<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Datasets &#8212; MNE 0.16.dev0 documentation</title>
    <link rel="stylesheet" href="../_static/bootstrap-sphinx.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.16.dev0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/javascript" src="../_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="../_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-3.3.7/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />

    <script type="text/javascript" src="../_static/copybutton.js"></script>


    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-37225609-1']);
    _gaq.push(['_trackPageview']);

    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>


    <link rel="stylesheet" href="../_static/style.css " type="text/css" />
    <link rel="stylesheet" href="../_static/font-awesome.css" type="text/css" />
    <link rel="stylesheet" href="../_static/flag-icon.css" type="text/css" />



    <script type="text/javascript">
    !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);
    js.id=id;js.src="https://platform.twitter.com/widgets.js";
    fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");
    </script>



    <script type="text/javascript">
    (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
    })();
    </script>


  </head>
  <body>

<div class="row devbar alert alert-danger">
This documentation is for <strong>development version 0.16.dev0</strong>.
</div>





  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="../index.html"><span><img src="../_static/mne_logo_small.png"></span>
           </a>
        <span class="navbar-text navbar-version pull-left"><b>0.16.dev0</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
                <li><a href="../getting_started.html">Install</a></li>
                <li><a href="../documentation.html">Documentation</a></li>
                <li><a href="../python_reference.html">API</a></li>
                <li><a href="../auto_examples/index.html">Examples</a></li>
                <li><a href="../contributing.html">Contribute</a></li>
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="../index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"></ul>
</li>
              
            
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
<div class="navbar-form navbar-right navbar-btn dropdown btn-group-sm" style="margin-left: 20px; margin-top: 5px; margin-bottom: 5px">
  <button type="button" class="btn btn-danger navbar-btn dropdown-toggle" id="dropdownMenu1" data-toggle="dropdown">
    v0.16.dev0
    <span class="caret"></span>
  </button>
  <ul class="dropdown-menu" aria-labelledby="dropdownMenu1">
    <li><a href="https://mne-tools.github.io/dev/index.html">Development</a></li>
    <li><a href="https://mne-tools.github.io/stable/index.html">v0.15 (stable)</a></li>
    <li><a href="https://mne-tools.github.io/0.14/index.html">v0.14</a></li>
    <li><a href="https://mne-tools.github.io/0.13/index.html">v0.13</a></li>
    <li><a href="https://mne-tools.github.io/0.12/index.html">v0.12</a></li>
    <li><a href="https://mne-tools.github.io/0.11/index.html">v0.11</a></li>
  </ul>
</div>


            
<form class="navbar-form navbar-right" action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          

        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/mne_logo_small.png" alt="Logo"/>
            </a></p><ul>
<li><a class="reference internal" href="#">Datasets</a><ul>
<li><a class="reference internal" href="#sample">Sample</a></li>
<li><a class="reference internal" href="#brainstorm">Brainstorm</a><ul>
<li><a class="reference internal" href="#auditory">Auditory</a></li>
<li><a class="reference internal" href="#resting-state">Resting state</a></li>
<li><a class="reference internal" href="#median-nerve">Median nerve</a></li>
</ul>
</li>
<li><a class="reference internal" href="#megsim">MEGSIM</a></li>
<li><a class="reference internal" href="#spm-faces">SPM faces</a></li>
<li><a class="reference internal" href="#eegbci-motor-imagery">EEGBCI motor imagery</a></li>
<li><a class="reference internal" href="#somatosensory">Somatosensory</a></li>
<li><a class="reference internal" href="#multimodal">Multimodal</a></li>
<li><a class="reference internal" href="#high-frequency-sef">High frequency SEF</a></li>
<li><a class="reference internal" href="#visual-92-object-categories">Visual 92 object categories</a></li>
<li><a class="reference internal" href="#mtrf-dataset">mTRF Dataset</a></li>
<li><a class="reference internal" href="#miscellaneous-datasets">Miscellaneous Datasets</a><ul>
<li><a class="reference internal" href="#ecog-dataset">ECoG Dataset</a></li>
</ul>
</li>
<li><a class="reference internal" href="#kiloword-dataset">Kiloword dataset</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

<form action="../search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
        </div>
      </div>
    <div class="col-md-12 content">
      
  <div class="section" id="datasets">
<span id="id1"></span><h1>Datasets<a class="headerlink" href="#datasets" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="contents">
<p class="topic-title first">Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#sample" id="id14">Sample</a></li>
<li><a class="reference internal" href="#brainstorm" id="id15">Brainstorm</a><ul>
<li><a class="reference internal" href="#auditory" id="id16">Auditory</a></li>
<li><a class="reference internal" href="#resting-state" id="id17">Resting state</a></li>
<li><a class="reference internal" href="#median-nerve" id="id18">Median nerve</a></li>
</ul>
</li>
<li><a class="reference internal" href="#megsim" id="id19">MEGSIM</a></li>
<li><a class="reference internal" href="#spm-faces" id="id20">SPM faces</a></li>
<li><a class="reference internal" href="#eegbci-motor-imagery" id="id21">EEGBCI motor imagery</a></li>
<li><a class="reference internal" href="#somatosensory" id="id22">Somatosensory</a></li>
<li><a class="reference internal" href="#multimodal" id="id23">Multimodal</a></li>
<li><a class="reference internal" href="#high-frequency-sef" id="id24">High frequency SEF</a></li>
<li><a class="reference internal" href="#visual-92-object-categories" id="id25">Visual 92 object categories</a></li>
<li><a class="reference internal" href="#mtrf-dataset" id="id26">mTRF Dataset</a></li>
<li><a class="reference internal" href="#miscellaneous-datasets" id="id27">Miscellaneous Datasets</a><ul>
<li><a class="reference internal" href="#ecog-dataset" id="id28">ECoG Dataset</a></li>
</ul>
</li>
<li><a class="reference internal" href="#kiloword-dataset" id="id29">Kiloword dataset</a></li>
<li><a class="reference internal" href="#references" id="id30">References</a></li>
</ul>
</div>
<p>All the dataset fetchers are available in <a class="reference internal" href="../python_reference.html#module-mne.datasets" title="mne.datasets"><code class="xref py py-mod docutils literal"><span class="pre">mne.datasets</span></code></a>. To download any of the datasets,
use the <code class="docutils literal"><span class="pre">data_path</span></code> (fetches full dataset) or the <code class="docutils literal"><span class="pre">load_data</span></code> (fetches dataset partially) functions.</p>
<div class="section" id="sample">
<h2><a class="toc-backref" href="#id14">Sample</a><a class="headerlink" href="#sample" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.sample.data_path.html#mne.datasets.sample.data_path" title="mne.datasets.sample.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.sample.data_path()</span></code></a></p>
<p><a class="reference internal" href="sample_dataset.html#ch-sample-data"><span class="std std-ref">The sample data set</span></a> is recorded using a 306-channel Neuromag vectorview system.</p>
<p>In this experiment, checkerboard patterns were presented to the subject
into the left and right visual field, interspersed by tones to the
left or right ear. The interval between the stimuli was 750 ms. Occasionally
a smiley face was presented at the center of the visual field.
The subject was asked to press a key with the right index finger
as soon as possible after the appearance of the face.</p>
<p>Once the <code class="docutils literal"><span class="pre">data_path</span></code> is known, its contents can be examined using <a class="reference internal" href="io.html#ch-convert"><span class="std std-ref">IO functions</span></a>.</p>
</div>
<div class="section" id="brainstorm">
<h2><a class="toc-backref" href="#id15">Brainstorm</a><a class="headerlink" href="#brainstorm" title="Permalink to this headline">¶</a></h2>
<p>Dataset fetchers for three Brainstorm tutorials are available. Users must agree to the
license terms of these datasets before downloading them. These files are recorded in a CTF 275 system.
The data is converted to <cite>fif</cite> format before being made available to MNE users. However, MNE-Python now supports
IO for the <cite>ctf</cite> format as well in addition to the C converter utilities. Please consult the <a class="reference internal" href="io.html#ch-convert"><span class="std std-ref">IO section</span></a> for details.</p>
<div class="section" id="auditory">
<h3><a class="toc-backref" href="#id16">Auditory</a><a class="headerlink" href="#auditory" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.brainstorm.bst_raw.data_path.html#mne.datasets.brainstorm.bst_raw.data_path" title="mne.datasets.brainstorm.bst_raw.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.brainstorm.bst_raw.data_path()</span></code></a>.</p>
<p>Details about the data can be found at the Brainstorm <a class="reference external" href="http://neuroimage.usc.edu/brainstorm/DatasetAuditory">auditory dataset tutorial</a>.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/datasets/plot_brainstorm_data.html#sphx-glr-auto-examples-datasets-plot-brainstorm-data-py"><span class="std std-ref">Brainstorm auditory dataset tutorial</span></a>: Partially replicates the original Brainstorm tutorial.</li>
</ul>
</div>
</div>
<div class="section" id="resting-state">
<h3><a class="toc-backref" href="#id17">Resting state</a><a class="headerlink" href="#resting-state" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.brainstorm.bst_resting.data_path.html#mne.datasets.brainstorm.bst_resting.data_path" title="mne.datasets.brainstorm.bst_resting.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.brainstorm.bst_resting.data_path()</span></code></a></p>
<p>Details can be found at the Brainstorm <a class="reference external" href="http://neuroimage.usc.edu/brainstorm/DatasetResting">resting state dataset tutorial</a>.</p>
</div>
<div class="section" id="median-nerve">
<h3><a class="toc-backref" href="#id18">Median nerve</a><a class="headerlink" href="#median-nerve" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.brainstorm.bst_raw.data_path.html#mne.datasets.brainstorm.bst_raw.data_path" title="mne.datasets.brainstorm.bst_raw.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.brainstorm.bst_raw.data_path()</span></code></a></p>
<p>Details can be found at the Brainstorm <a class="reference external" href="http://neuroimage.usc.edu/brainstorm/DatasetMedianNerveCtf">median nerve dataset tutorial</a>.</p>
</div>
</div>
<div class="section" id="megsim">
<h2><a class="toc-backref" href="#id19">MEGSIM</a><a class="headerlink" href="#megsim" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.megsim.load_data.html#mne.datasets.megsim.load_data" title="mne.datasets.megsim.load_data"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.megsim.load_data()</span></code></a></p>
<p>This dataset contains experimental and simulated MEG data. To load data from this dataset, do:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mne.io</span> <span class="k">import</span> <span class="n">Raw</span>
<span class="kn">from</span> <span class="nn">mne.datasets.megsim</span> <span class="k">import</span> <span class="n">load_data</span>
<span class="n">raw_fnames</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">condition</span><span class="o">=</span><span class="s1">&#39;visual&#39;</span><span class="p">,</span> <span class="n">data_format</span><span class="o">=</span><span class="s1">&#39;raw&#39;</span><span class="p">,</span> <span class="n">data_type</span><span class="o">=</span><span class="s1">&#39;experimental&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">raw</span> <span class="o">=</span> <span class="n">Raw</span><span class="p">(</span><span class="n">raw_fnames</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>Detailed description of the dataset can be found in the related publication <a class="footnote-reference" href="#id8" id="id2">[1]</a>.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/datasets/plot_megsim_data.html#sphx-glr-auto-examples-datasets-plot-megsim-data-py"><span class="std std-ref">MEGSIM experimental and simulation datasets</span></a></li>
</ul>
</div>
</div>
<div class="section" id="spm-faces">
<h2><a class="toc-backref" href="#id20">SPM faces</a><a class="headerlink" href="#spm-faces" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.spm_face.data_path.html#mne.datasets.spm_face.data_path" title="mne.datasets.spm_face.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.spm_face.data_path()</span></code></a></p>
<p>The <a class="reference external" href="http://www.fil.ion.ucl.ac.uk/spm/data/mmfaces/">SPM faces dataset</a> contains EEG, MEG and fMRI recordings on face perception.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/datasets/plot_spm_faces_dataset.html#sphx-glr-auto-examples-datasets-plot-spm-faces-dataset-py"><span class="std std-ref">From raw data to dSPM on SPM Faces dataset</span></a> Full pipeline including artifact removal, epochs averaging, forward model computation and source reconstruction using dSPM on the contrast: “faces - scrambled”.</li>
</ul>
</div>
</div>
<div class="section" id="eegbci-motor-imagery">
<h2><a class="toc-backref" href="#id21">EEGBCI motor imagery</a><a class="headerlink" href="#eegbci-motor-imagery" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.eegbci.load_data.html#mne.datasets.eegbci.load_data" title="mne.datasets.eegbci.load_data"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.eegbci.load_data()</span></code></a></p>
<p>The EEGBCI dataset is documented in <a class="footnote-reference" href="#id9" id="id3">[2]</a>. The data set is available at PhysioNet <a class="footnote-reference" href="#id10" id="id4">[3]</a>.
The dataset contains 64-channel EEG recordings from 109 subjects and 14 runs on each subject in EDF+ format.
The recordings were made using the BCI2000 system. To load a subject, do:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mne.io</span> <span class="k">import</span> <span class="n">concatenate_raws</span><span class="p">,</span> <span class="n">read_raw_edf</span>
<span class="kn">from</span> <span class="nn">mne.datasets</span> <span class="k">import</span> <span class="n">eegbci</span>
<span class="n">raw_fnames</span> <span class="o">=</span> <span class="n">eegbci</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">subject</span><span class="p">,</span> <span class="n">runs</span><span class="p">)</span>
<span class="n">raws</span> <span class="o">=</span> <span class="p">[</span><span class="n">read_raw_edf</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">preload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">raw_fnames</span><span class="p">]</span>
<span class="n">raw</span> <span class="o">=</span> <span class="n">concatenate_raws</span><span class="p">(</span><span class="n">raws</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decoding/plot_decoding_csp_eeg.html#sphx-glr-auto-examples-decoding-plot-decoding-csp-eeg-py"><span class="std std-ref">Motor imagery decoding from EEG data using the Common Spatial Pattern (CSP)</span></a></li>
</ul>
</div>
<p>Do not hesitate to contact MNE-Python developers on the
<a class="reference external" href="http://mail.nmr.mgh.harvard.edu/mailman/listinfo/mne_analysis">MNE mailing list</a>
to discuss the possibility to add more publicly available datasets.</p>
</div>
<div class="section" id="somatosensory">
<h2><a class="toc-backref" href="#id22">Somatosensory</a><a class="headerlink" href="#somatosensory" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.somato.data_path.html#mne.datasets.somato.data_path" title="mne.datasets.somato.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.somato.data_path()</span></code></a></p>
<p>This dataset contains somatosensory data with event-related synchronizations
(ERS) and desynchronizations (ERD).</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_tutorials/plot_sensors_time_frequency.html#sphx-glr-auto-tutorials-plot-sensors-time-frequency-py"><span class="std std-ref">Frequency and time-frequency sensors analysis</span></a></li>
</ul>
</div>
</div>
<div class="section" id="multimodal">
<h2><a class="toc-backref" href="#id23">Multimodal</a><a class="headerlink" href="#multimodal" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.multimodal.data_path.html#mne.datasets.multimodal.data_path" title="mne.datasets.multimodal.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.multimodal.data_path()</span></code></a></p>
<p>This dataset contains a single subject recorded at Otaniemi (Aalto University)
with auditory, visual, and somatosensory stimuli.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/io/plot_elekta_epochs.html#sphx-glr-auto-examples-io-plot-elekta-epochs-py"><span class="std std-ref">Getting averaging info from .fif files</span></a></li>
</ul>
</div>
</div>
<div class="section" id="high-frequency-sef">
<h2><a class="toc-backref" href="#id24">High frequency SEF</a><a class="headerlink" href="#high-frequency-sef" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.hf_sef.data_path.html#mne.datasets.hf_sef.data_path" title="mne.datasets.hf_sef.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.hf_sef.data_path()</span></code></a></p>
<p>This dataset contains somatosensory evoked fields (median nerve stimulation)
with thousands of epochs. It was recorded with an Elekta TRIUX MEG device at
a sampling frequency of 3 kHz. The dataset is suitable for investigating
high-frequency somatosensory responses. Data from two subjects are included
with MRI images in DICOM format and FreeSurfer reconstructions.</p>
</div>
<div class="section" id="visual-92-object-categories">
<h2><a class="toc-backref" href="#id25">Visual 92 object categories</a><a class="headerlink" href="#visual-92-object-categories" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.visual_92_categories.data_path.html#mne.datasets.visual_92_categories.data_path" title="mne.datasets.visual_92_categories.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.visual_92_categories.data_path()</span></code></a>.</p>
<p>This dataset is recorded using a 306-channel Neuromag vectorview system.</p>
<p>Experiment consisted in the visual presentation of 92 images of human, animal
and inanimate objects either natural or artificial <a class="footnote-reference" href="#id11" id="id5">[4]</a>. Given the high number
of conditions this dataset is well adapted to an approach based on
Representational Similarity Analysis (RSA).</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decoding/decoding_rsa.html#sphx-glr-auto-examples-decoding-decoding-rsa-py"><span class="std std-ref">Representational Similarity Analysis (RSA)</span></a>: Partially replicates the results from Cichy et al. (2014).</li>
</ul>
</div>
</div>
<div class="section" id="mtrf-dataset">
<h2><a class="toc-backref" href="#id26">mTRF Dataset</a><a class="headerlink" href="#mtrf-dataset" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.mtrf.data_path.html#mne.datasets.mtrf.data_path" title="mne.datasets.mtrf.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.mtrf.data_path()</span></code></a>.</p>
<p>This dataset contains 128 channel EEG as well as natural speech stimulus features,
which is also available <a class="reference external" href="https://sourceforge.net/projects/aespa/files/">here</a>.</p>
<p>The experiment consisted of subjects listening to natural speech.
The dataset contains several feature representations of the speech stimulus,
suitable for using to fit continuous regression models of neural activity.
More details and a description of the package can be found in <a class="footnote-reference" href="#id12" id="id6">[5]</a>.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/decoding/plot_receptive_field.html#sphx-glr-auto-examples-decoding-plot-receptive-field-py"><span class="std std-ref">Receptive Field Estimation and Prediction</span></a>: Partially replicates the results from Crosse et al. (2016).</li>
</ul>
</div>
</div>
<div class="section" id="miscellaneous-datasets">
<h2><a class="toc-backref" href="#id27">Miscellaneous Datasets</a><a class="headerlink" href="#miscellaneous-datasets" title="Permalink to this headline">¶</a></h2>
<p>These datasets are used for specific purposes in the documentation and in
general are not useful for separate analyses.</p>
<div class="section" id="ecog-dataset">
<h3><a class="toc-backref" href="#id28">ECoG Dataset</a><a class="headerlink" href="#ecog-dataset" title="Permalink to this headline">¶</a></h3>
<p><a class="reference internal" href="../generated/mne.datasets.misc.data_path.html#mne.datasets.misc.data_path" title="mne.datasets.misc.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.misc.data_path()</span></code></a>. Data exists at <code class="docutils literal"><span class="pre">/ecog/sample_ecog.mat</span></code>.</p>
<p>This dataset contains a sample Electrocorticography (ECoG) dataset. It includes
a single grid of electrodes placed over the temporal lobe during an auditory
listening task. This dataset is primarily used to demonstrate visualization
functions in MNE and does not contain useful metadata for analysis.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/visualization/plot_3d_to_2d.html#sphx-glr-auto-examples-visualization-plot-3d-to-2d-py"><span class="std std-ref">How to convert 3D electrode positions to a 2D image.</span></a>: Demonstrates
how to project a 3D electrode location onto a 2D image, a common procedure
in electrocorticography.</li>
</ul>
</div>
</div>
</div>
<div class="section" id="kiloword-dataset">
<h2><a class="toc-backref" href="#id29">Kiloword dataset</a><a class="headerlink" href="#kiloword-dataset" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="../generated/mne.datasets.kiloword.data_path.html#mne.datasets.kiloword.data_path" title="mne.datasets.kiloword.data_path"><code class="xref py py-func docutils literal"><span class="pre">mne.datasets.kiloword.data_path()</span></code></a>.</p>
<p>This dataset consists of averaged EEG data from 75 subjects performing a lexical decision
task on 960 English words <a class="footnote-reference" href="#id13" id="id7">[6]</a>. The words are richly annotated, and can be used for e.g.
multiple regression estimation of EEG correlates of printed word processing.</p>
</div>
<div class="section" id="references">
<h2><a class="toc-backref" href="#id30">References</a><a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<table class="docutils footnote" frame="void" id="id8" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>Aine CJ, Sanfratello L, Ranken D, Best E, MacArthur JA, Wallace T, Gilliam K, Donahue CH, Montano R, Bryant JE, Scott A, Stephen JM (2012) MEG-SIM: A Web Portal for Testing MEG Analysis Methods using Realistic Simulated and Empirical Data. Neuroinform 10:141-158</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id9" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id3">[2]</a></td><td>Schalk, G., McFarland, D.J., Hinterberger, T., Birbaumer, N., Wolpaw, J.R. (2004) BCI2000: A General-Purpose Brain-Computer Interface (BCI) System. IEEE TBME 51(6):1034-1043</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id10" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[3]</a></td><td>Goldberger AL, Amaral LAN, Glass L, Hausdorff JM, Ivanov PCh, Mark RG, Mietus JE, Moody GB, Peng C-K, Stanley HE. (2000) PhysioBank, PhysioToolkit, and PhysioNet: Components of a New Research Resource for Complex Physiologic Signals. Circulation 101(23):e215-e220</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[4]</a></td><td>Cichy, R. M., Pantazis, D., &amp; Oliva, A. Resolving human object recognition in space and time. Nature Neuroscience (2014): 17(3), 455-462</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id12" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id6">[5]</a></td><td>Crosse, M. J., Di Liberto, G. M., Bednar, A., &amp; Lalor, E. C. The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli. Frontiers in Human Neuroscience (2016): 10.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id7">[6]</a></td><td>Dufau, S., Grainger, J., Midgley, KJ., Holcomb, PJ. A thousand words are worth a picture: Snapshots of printed-word processing in an event-related potential megastudy. Psychological science, 2015</td></tr>
</tbody>
</table>
</div>
</div>


    </div>
    
  </div>
</div>
<footer class="footer">
  <div class="container"><img src="../_static/institutions.png" alt="Institutions"></div>
  <div class="container">
    <ul class="list-inline">
      <li><a href="https://github.com/mne-tools/mne-python">GitHub</a></li>
      <li>·</li>
      <li><a href="https://mail.nmr.mgh.harvard.edu/mailman/listinfo/mne_analysis">Mailing list</a></li>
      <li>·</li>
      <li><a href="https://gitter.im/mne-tools/mne-python">Gitter</a></li>
      <li>·</li>
      <li><a href="whats_new.html">What's new</a></li>
      <li>·</li>
      <li><a href="faq.html#cite">Cite MNE</a></li>
      <li class="pull-right"><a href="#">Back to top</a></li>
    </ul>
    <p>&copy; Copyright 2012-2017, MNE Developers. Last updated on 2017-10-27.</p>
  </div>
</footer>
  </body>
</html>