{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nSource alignment and coordinate frames\n======================================\n\nThe aim of this tutorial is to show how to visually assess that the data are\nwell aligned in space for computing the forward solution, and understand\nthe different coordinate frames involved in this process.\n   :depth: 2\n\nLet's start out by loading some data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os.path as op\n\nimport numpy as np\n\nimport mne\nfrom mne.datasets import sample\n\nprint(__doc__)\n\ndata_path = sample.data_path()\nsubjects_dir = op.join(data_path, 'subjects')\nraw_fname = op.join(data_path, 'MEG', 'sample', 'sample_audvis_raw.fif')\ntrans_fname = op.join(data_path, 'MEG', 'sample',\n                      'sample_audvis_raw-trans.fif')\nraw = mne.io.read_raw_fif(raw_fname)\ntrans = mne.read_trans(trans_fname)\nsrc = mne.read_source_spaces(op.join(subjects_dir, 'sample', 'bem',\n                                     'sample-oct-6-src.fif'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Understanding coordinate frames\n-------------------------------\nFor M/EEG source imaging, there are three **coordinate frames** (further\nexplained in the next section) that we must bring into alignment using two 3D\n`transformation matrices <rotation and translation matrix_>`_\nthat define how to rotate and translate points in one coordinate frame\nto their equivalent locations in another.\n\n:func:`mne.viz.plot_alignment` is a very useful function for inspecting\nthese transformations, and the resulting alignment of EEG sensors, MEG\nsensors, brain sources, and conductor models. If the ``subjects_dir`` and\n``subject`` parameters are provided, the function automatically looks for the\nFreesurfer MRI surfaces to show from the subject's folder.\n\nWe can use the ``show_axes`` argument to see the various coordinate frames\ngiven our transformation matrices. These are shown by axis arrows for each\ncoordinate frame:\n\n* shortest arrow is (**R**)ight/X\n* medium is forward/(**A**)nterior/Y\n* longest is up/(**S**)uperior/Z\n\ni.e., a **RAS** coordinate system in each case. We can also set\nthe ``coord_frame`` argument to choose which coordinate\nframe the camera should initially be aligned with.\n\nLet's take a look:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = mne.viz.plot_alignment(raw.info, trans=trans, subject='sample',\n                             subjects_dir=subjects_dir, surfaces='head-dense',\n                             show_axes=True, dig=True, eeg=[], meg='sensors',\n                             coord_frame='meg')\nmne.viz.set_3d_view(fig, 45, 90, distance=0.6, focalpoint=(0., 0., 0.))\nprint('Distance from head origin to MEG origin: %0.1f mm'\n      % (1000 * np.linalg.norm(raw.info['dev_head_t']['trans'][:3, 3])))\nprint('Distance from head origin to MRI origin: %0.1f mm'\n      % (1000 * np.linalg.norm(trans['trans'][:3, 3])))\ndists = mne.dig_mri_distances(raw.info, trans, 'sample',\n                              subjects_dir=subjects_dir)\nprint('Distance from %s digitized points to head surface: %0.1f mm'\n      % (len(dists), 1000 * np.mean(dists)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coordinate frame definitions\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n.. raw:: html\n\n   <style>\n   .pink {color:DarkSalmon; font-weight:bold}\n   .blue {color:DeepSkyBlue; font-weight:bold}\n   .gray {color:Gray; font-weight:bold}\n   .magenta {color:Magenta; font-weight:bold}\n   .purple {color:Indigo; font-weight:bold}\n   .green {color:LimeGreen; font-weight:bold}\n   .red {color:Red; font-weight:bold}\n   </style>\n\n.. role:: pink\n.. role:: blue\n.. role:: gray\n.. role:: magenta\n.. role:: purple\n.. role:: green\n.. role:: red\n\n1. Neuromag/Elekta/MEGIN head coordinate frame (\"head\", :pink:`pink axes`)\n     The head coordinate frame is defined through the coordinates of\n     anatomical landmarks on the subject's head: Usually the Nasion (`NAS`_),\n     and the left and right preauricular points (`LPA`_ and `RPA`_).\n     Different MEG manufacturers may have different definitions of the\n     coordinate head frame. A good overview can be seen in the\n     `FieldTrip FAQ on coordinate systems`_.\n\n     For Neuromag/Elekta/MEGIN, the head coordinate frame is defined by the\n     intersection of\n\n     1. the line between the LPA (:red:`red sphere`) and RPA\n        (:purple:`purple sphere`), and\n     2. the line perpendicular to this LPA-RPA line one that goes through\n        the Nasion (:green:`green sphere`).\n\n     The axes are oriented as **X** origin\u2192RPA, **Y** origin\u2192NAS,\n     **Z** origin\u2192upward (orthogonal to X and Y).\n\n     .. note:: The required 3D coordinates for defining the head coordinate\n               frame (NAS, LPA, RPA) are measured at a stage separate from\n               the MEG data recording. There exist numerous devices to\n               perform such measurements, usually called \"digitizers\". For\n               example, see the devices by the company `Polhemus`_.\n\n2. MEG device coordinate frame (\"meg\", :blue:`blue axes`)\n     The MEG device coordinate frame is defined by the respective MEG\n     manufacturers. All MEG data is acquired with respect to this coordinate\n     frame. To account for the anatomy and position of the subject's head, we\n     use so-called head position indicator (HPI) coils. The HPI coils are\n     placed at known locations on the scalp of the subject and emit\n     high-frequency magnetic fields used to coregister the head coordinate\n     frame with the device coordinate frame.\n\n     From the Neuromag/Elekta/MEGIN user manual:\n\n         The origin of the device coordinate system is located at the center\n         of the posterior spherical section of the helmet with X axis going\n         from left to right and Y axis pointing front. The Z axis is, again\n         normal to the plane with positive direction up.\n\n     .. note:: The HPI coils are shown as :magenta:`magenta spheres`.\n               Coregistration happens at the beginning of the recording and\n               the data is stored in ``raw.info['dev_head_t']``.\n\n3. MRI coordinate frame (\"mri\", :gray:`gray axes`)\n     Defined by Freesurfer, the MRI (surface RAS) origin is at the\n     center of a 256\u00d7256\u00d7256 1mm anisotropic volume (may not be in the center\n     of the head).\n\n     .. note:: We typically align the MRI coordinate frame to the head\n               coordinate frame through a `rotation and translation matrix`_,\n               that we refer to in MNE as ``trans``.\n\nA bad example\n-------------\nLet's try using ``trans=None``, which (incorrectly!) equates the MRI\nand head coordinate frames.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mne.viz.plot_alignment(raw.info, trans=None, subject='sample', src=src,\n                       subjects_dir=subjects_dir, dig=True,\n                       surfaces=['head-dense', 'white'], coord_frame='meg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is quite clear that the MRI surfaces (head, brain) are not well aligned\nto the head digitization points (dots).\n\nA good example\n--------------\nHere is the same plot, this time with the ``trans`` properly defined\n(using a precomputed matrix).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mne.viz.plot_alignment(raw.info, trans=trans, subject='sample',\n                       src=src, subjects_dir=subjects_dir, dig=True,\n                       surfaces=['head-dense', 'white'], coord_frame='meg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the head\u2194MRI ``trans`` using the GUI\n---------------------------------------------\nYou can try creating the head\u2194MRI transform yourself using\n:func:`mne.gui.coregistration`.\n\n* First you must load the digitization data from the raw file\n  (``Head Shape Source``). The MRI data is already loaded if you provide the\n  ``subject`` and ``subjects_dir``. Toggle ``Always Show Head Points`` to see\n  the digitization points.\n* To set the landmarks, toggle ``Edit`` radio button in ``MRI Fiducials``.\n* Set the landmarks by clicking the radio button (LPA, Nasion, RPA) and then\n  clicking the corresponding point in the image.\n* After doing this for all the landmarks, toggle ``Lock`` radio button. You\n  can omit outlier points, so that they don't interfere with the finetuning.\n\n  .. note:: You can save the fiducials to a file and pass\n            ``mri_fiducials=True`` to plot them in\n            :func:`mne.viz.plot_alignment`. The fiducials are saved to the\n            subject's bem folder by default.\n* Click ``Fit Head Shape``. This will align the digitization points to the\n  head surface. Sometimes the fitting algorithm doesn't find the correct\n  alignment immediately. You can try first fitting using LPA/RPA or fiducials\n  and then align according to the digitization. You can also finetune\n  manually with the controls on the right side of the panel.\n* Click ``Save As...`` (lower right corner of the panel), set the filename\n  and read it with :func:`mne.read_trans`.\n\nFor more information, see step by step instructions\n`in these slides\n<https://www.slideshare.net/mne-python/mnepython-coregistration>`_.\nUncomment the following line to align the data yourself.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# mne.gui.coregistration(subject='sample', subjects_dir=subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAlignment without MRI\n---------------------\nThe surface alignments above are possible if you have the surfaces available\nfrom Freesurfer. :func:`mne.viz.plot_alignment` automatically searches for\nthe correct surfaces from the provided ``subjects_dir``. Another option is\nto use a `spherical conductor model <eeg_sphere_model>`. It is\npassed through ``bem`` parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sphere = mne.make_sphere_model(info=raw.info, r0='auto', head_radius='auto')\nsrc = mne.setup_volume_source_space(sphere=sphere, pos=10.)\nmne.viz.plot_alignment(\n    raw.info, eeg='projected', bem=sphere, src=src, dig=True,\n    surfaces=['brain', 'outer_skin'], coord_frame='meg', show_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to use :func:`mne.gui.coregistration`\nto warp a subject (usually ``fsaverage``) to subject digitization data, see\n`these slides\n<https://www.slideshare.net/mne-python/mnepython-scale-mri>`_.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}