{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Visualize source time courses (stcs)\n\nThis tutorial focuses on visualization of :term:`source estimates <STC>`.\n\n## Surface Source Estimates\nFirst, we get the paths for the evoked data and the source time courses (stcs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\n\nimport mne\nfrom mne.datasets import sample, fetch_hcp_mmp_parcellation\nfrom mne.minimum_norm import apply_inverse, read_inverse_operator\nfrom mne import read_evokeds\n\ndata_path = sample.data_path()\nmeg_path = data_path / 'MEG' / 'sample'\nsubjects_dir = data_path / 'subjects'\n\nfname_evoked = meg_path / 'sample_audvis-ave.fif'\nfname_stc = meg_path / 'sample_audvis-meg'\nfetch_hcp_mmp_parcellation(subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we read the stc from file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stc = mne.read_source_estimate(fname_stc, subject='sample')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a :class:`SourceEstimate <mne.SourceEstimate>` object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(stc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The SourceEstimate object is in fact a *surface* source estimate. MNE also\nsupports volume-based source estimates but more on that later.\n\nWe can plot the source estimate using the\n:func:`stc.plot <mne.SourceEstimate.plot>` just as in other MNE\nobjects. Note that for this visualization to work, you must have ``PyVista``\ninstalled on your machine.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initial_time = 0.1\nbrain = stc.plot(subjects_dir=subjects_dir, initial_time=initial_time,\n                 clim=dict(kind='value', lims=[3, 6, 9]),\n                 smoothing_steps=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also morph it to fsaverage and visualize it using a flatmap.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stc_fs = mne.compute_source_morph(stc, 'sample', 'fsaverage', subjects_dir,\n                                  smooth=5, verbose='error').apply(stc)\nbrain = stc_fs.plot(subjects_dir=subjects_dir, initial_time=initial_time,\n                    clim=dict(kind='value', lims=[3, 6, 9]),\n                    surface='flat', hemi='both', size=(1000, 500),\n                    smoothing_steps=5, time_viewer=False,\n                    add_data_kwargs=dict(\n                        colorbar_kwargs=dict(label_font_size=10)))\n\n# to help orient us, let's add a parcellation (red=auditory, green=motor,\n# blue=visual)\nbrain.add_annotation('HCPMMP1_combined', borders=2)\n\n# You can save a movie like the one on our documentation website with:\n# brain.save_movie(time_dilation=20, tmin=0.05, tmax=0.16,\n#                  interpolation='linear', framerate=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that here we used ``initial_time=0.1``, but we can also browse through\ntime using ``time_viewer=True``.\n\nIn case ``PyVista`` is not available, we also offer a ``matplotlib``\nbackend. Here we use verbose='error' to ignore a warning that not all\nvertices were used in plotting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mpl_fig = stc.plot(subjects_dir=subjects_dir, initial_time=initial_time,\n                   backend='matplotlib', verbose='error', smoothing_steps=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Volume Source Estimates\nWe can also visualize volume source estimates (used for deep structures).\n\nLet us load the sensor-level evoked data. We select the MEG channels\nto keep things simple.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evoked = read_evokeds(fname_evoked, condition=0, baseline=(None, 0))\nevoked.pick_types(meg=True, eeg=False).crop(0.05, 0.15)\n# this risks aliasing, but these data are very smooth\nevoked.decimate(10, verbose='error')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can load the precomputed inverse operator from a file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fname_inv = meg_path / 'sample_audvis-meg-vol-7-meg-inv.fif'\ninv = read_inverse_operator(fname_inv)\nsrc = inv['src']\nmri_head_t = inv['mri_head_t']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The source estimate is computed using the inverse operator and the\nsensor-space data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "snr = 3.0\nlambda2 = 1.0 / snr ** 2\nmethod = \"dSPM\"  # use dSPM method (could also be MNE or sLORETA)\nstc = apply_inverse(evoked, inv, lambda2, method)\ndel inv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time, we have a different container\n(:class:`VolSourceEstimate <mne.VolSourceEstimate>`) for the source time\ncourse.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(stc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This too comes with a convenient plot method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stc.plot(src, subject='sample', subjects_dir=subjects_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this visualization, ``nilearn`` must be installed.\nThis visualization is interactive. Click on any of the anatomical slices\nto explore the time series. Clicking on any time point will bring up the\ncorresponding anatomical map.\n\nWe could visualize the source estimate on a glass brain. Unlike the previous\nvisualization, a glass brain does not show us one slice but what we would\nsee if the brain was transparent like glass, and\n:term:`maximum intensity projection`) is used:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stc.plot(src, subject='sample', subjects_dir=subjects_dir, mode='glass_brain')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also extract label time courses using volumetric atlases. Here we'll\nuse the built-in ``aparc+aseg.mgz``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fname_aseg = subjects_dir / 'sample' / 'mri' / 'aparc+aseg.mgz'\nlabel_names = mne.get_volume_labels_from_aseg(fname_aseg)\nlabel_tc = stc.extract_label_time_course(fname_aseg, src=src)\n\nlidx, tidx = np.unravel_index(np.argmax(label_tc), label_tc.shape)\nfig, ax = plt.subplots(1)\nax.plot(stc.times, label_tc.T, 'k', lw=1., alpha=0.5)\nxy = np.array([stc.times[tidx], label_tc[lidx, tidx]])\nxytext = xy + [0.01, 1]\nax.annotate(\n    label_names[lidx], xy, xytext, arrowprops=dict(arrowstyle='->'), color='r')\nax.set(xlim=stc.times[[0, -1]], xlabel='Time (s)', ylabel='Activation')\nfor key in ('right', 'top'):\n    ax.spines[key].set_visible(False)\nfig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can plot several labels with the most activation in their time course\nfor a more fine-grained view of the anatomical loci of activation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels = [label_names[idx] for idx in np.argsort(label_tc.max(axis=1))[:7]\n          if 'unknown' not in label_names[idx].lower()]  # remove catch-all\nbrain = mne.viz.Brain('sample', hemi='both', surf='pial', alpha=0.5,\n                      cortex='low_contrast', subjects_dir=subjects_dir)\nbrain.add_volume_labels(aseg='aparc+aseg', labels=labels)\nbrain.show_view(azimuth=250, elevation=40, distance=400)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can project these label time courses back to their original\nlocations and see how the plot has been smoothed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stc_back = mne.labels_to_stc(fname_aseg, label_tc, src=src)\nstc_back.plot(src, subjects_dir=subjects_dir, mode='glass_brain')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector Source Estimates\nIf we choose to use ``pick_ori='vector'`` in\n:func:`apply_inverse <mne.minimum_norm.apply_inverse>`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fname_inv = (\n    data_path / 'MEG' / 'sample' / 'sample_audvis-meg-oct-6-meg-inv.fif'\n)\ninv = read_inverse_operator(fname_inv)\nstc = apply_inverse(evoked, inv, lambda2, 'dSPM', pick_ori='vector')\nbrain = stc.plot(subject='sample', subjects_dir=subjects_dir,\n                 initial_time=initial_time, brain_kwargs=dict(\n                     silhouette=True), smoothing_steps=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dipole fits\nFor computing a dipole fit, we need to load the noise covariance, the BEM\nsolution, and the coregistration transformation files. Note that for the\nother methods, these were already used to generate the inverse operator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fname_cov = meg_path / 'sample_audvis-cov.fif'\nfname_bem = subjects_dir / 'sample' / 'bem' / 'sample-5120-bem-sol.fif'\nfname_trans = meg_path / 'sample_audvis_raw-trans.fif'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dipoles are fit independently for each time point, so let us crop our time\nseries to visualize the dipole fit for the time point of interest.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evoked.crop(0.1, 0.1)\ndip = mne.fit_dipole(evoked, fname_cov, fname_bem, fname_trans)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can visualize the dipole.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dip.plot_locations(fname_trans, 'sample', subjects_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}