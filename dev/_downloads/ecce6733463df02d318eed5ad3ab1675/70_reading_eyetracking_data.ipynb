{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Importing Data from Eyetracking devices\n\nEyetracking devices record a persons point of gaze, usually in relation to a\nscreen. Typically, gaze position (also referred to as eye or pupil position)\nand pupil size are recorded as separate channels. This section describes how to\nread data from supported eyetracking manufacturers.\n\nMNE-Python provides functions for reading eyetracking data. When possible,\nMNE-Python will internally convert and store eyetracking data according to an\nSI unit (for example radians for position data, and meters for pupil size).\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>If you have eye tracking data in a format that MNE does not support\n          yet, you can try reading it using other tools and create an MNE\n          object from a numpy array. Then you can use\n          :func:`mne.preprocessing.eyetracking.set_channel_types_eyetrack`\n          to assign the correct eyetrack channel types.</p></div>\n\n.. seealso:: Some MNE functions may not be available to eyetracking and other\n             physiological data, because MNE does not consider them to be data\n             channels. See the `glossary` for more information.\n\n\n## SR Research (Eyelink) (.asc)\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>MNE-Python currently only supports reading Eyelink eyetracking data\n          stored in the ASCII (.asc) format.</p></div>\n\nEyelink recordings are stored in the Eyelink Data Format (EDF; .edf), which are\nbinary files and thus relatively complex to support. To make the data in EDF\nfiles accessible, Eyelink provides the application EDF2ASC, which converts EDF\nfiles to a plain text ASCII format (.asc). These files can be imported\ninto MNE using :func:`mne.io.read_raw_eyelink`.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The Eyelink Data Format (EDF), should not be confused\n          with the European Data Format, the common EEG data format that also\n          uses the .edf extension.</p></div>\n\nSupported measurement types from Eyelink files include eye position, pupil\nsize, saccadic velocity, resolution, and head position (for recordings\ncollected in remote mode). Eyelink files often report ocular events (blinks,\nsaccades, and fixations), MNE will store these events as `mne.Annotations`.\nBlink annotation descriptions will be ``'BAD_blink'``. For more information\non the various measurement types that can be present in Eyelink files. read below.\n\n### Eye Position Data\n\nEyelink samples can report eye position data in pixels, units of visual\ndegrees, or as raw pupil coordinates. Samples are written as (x, y) coordinate\npairs (or two pairs for binocular data). The type of position data present in\nan ASCII file will be detected automatically by MNE. The three types of\nposition data are explained below.\n\n#### Gaze\nGaze position data report the estimated (x, y) pixel coordinates of the\nparticipants's gaze on the stimulus screen, compensating for head position\nchanges and distance from  the screen. This datatype may be preferable if you\nare interested in knowing where the participant was looking at on the stimulus\nscreen. The default (0, 0) location for Eyelink systems is at the top left of\nthe screen.\n\nThis may be best demonstrated with an example. In the file plotted below,\neyetracking data was recorded while the participant read text on a display.\nIn this file, as the participant read the each line from left to right, the\nx-coordinate increased. When the participant moved their gaze down to read a\nnew line, the y-coordinate *increased*, which is why the ``ypos_right`` channel\nin the plot below increases over time (for example, at about 4-seconds, and\nat about 8-seconds).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne.datasets import misc\nfrom mne.io import read_raw_eyelink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fpath = misc.data_path() / \"eyetracking\" / \"eyelink\"\nraw = read_raw_eyelink(fpath / \"px_textpage_ws.asc\", create_annotations=[\"blinks\"])\ncustom_scalings = dict(eyegaze=1e3)\nraw.pick(picks=\"eyetrack\").plot(scalings=custom_scalings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. important:: The (0, 0) pixel coordinates are at the top-left of the\n              trackable area of the screen. Gaze towards lower areas of the\n              the screen will yield a relatively higher y-coordinate.\n\nNote that we passed a custom `dict` to the ``'scalings'`` argument of\n`mne.io.Raw.plot`. This is because MNE's default plot scalings for eye\nposition data are calibrated for HREF data, which are stored in radians\n(read below).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Head-Referenced Eye Angle (HREF)\n\nHREF position data measures eye rotation angles relative to the head. It does\nnot take into account changes in subject head position and angle, or distance\nfrom the stimulus screen. This datatype might be preferable for analyses\nthat are interested in eye movement velocities and amplitudes, or for\nsimultaneous and EEG/MEG eyetracking recordings where eye position data are\nused to identify EOG artifacts.\n\nHREF coordinates are stored in the ASCII file as integer values, with 260 or\nmore units per visual degree, however MNE will convert and store these\ncoordinates in radians. The (0, 0) point of HREF data is arbitrary, as the\nrelationship between the screen position and the coordinates changes as the\nsubject's head moves.\n\nBelow is the same text reading recording that we plotted above, except a new\nASCII file was generated, this time using HREF eye position data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fpath = misc.data_path() / \"eyetracking\" / \"eyelink\"\nraw = read_raw_eyelink(fpath / \"HREF_textpage_ws.asc\", create_annotations=[\"blinks\"])\nraw.pick(picks=\"eyetrack\").plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pupil Position\n\nPupil position data contains (x, y) coordinate pairs from the eye camera.\nIt has not been converted to pixels (gaze) or eye angles (HREF). Most use\ncases do not require this data type, and caution should be taken when\nanalyzing raw pupil positions. Note that when plotting data from a\n``Raw`` object containing raw pupil position data, the plot scalings\nwill likely be incorrect. You can pass custom scalings into the ``scalings``\nparameter of `mne.io.Raw.plot` so that the signals are legible when plotting.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-danger\"><h4>Warning</h4><p>If a calibration was not performed prior to data collection, the\n             EyeLink system cannot convert raw pupil position data to pixels\n             (gaze) or eye angle (HREF).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pupil Size Data\nPupil size is measured by the EyeLink system at up to 500 samples per second.\nIt may be reported as pupil *area*, or pupil *diameter* (i.e. the diameter\nof a circle/ellipse model fit to the pupil area).\nWhich of these datatypes you get is specified by your recording- and/or your\nEDF2ASC settings. The pupil size data is not calibrated and reported in\narbitrary units. Typical pupil *area* data range between 800 to 2000 units,\nwith a precision of 1 unit, while pupil *diameter* data range between\n1800-3000 units.\n\n### Velocity, resolution, and head position data\nEyelink files can produce data on saccadic velocity, resolution, and head\nposition for each sample in the file. MNE will read in these data if they are\npresent in the file, but will label their channel types as ``'misc'``.\n\n .. warning:: Eyelink's EDF2ASC API allows for modification of the data\n             and format that is converted to ASCII. However, MNE-Python\n             assumes a specific structure, which the default parameters of\n             EDF2ASC follow. ASCII files should be tab-deliminted, and both\n             Samples and Events should be output. If the data were recorded\n             at 2000Hz, timestamps should be floating point numbers. Manual\n             modification of ASCII conversion via EDF2ASC is not recommended.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}